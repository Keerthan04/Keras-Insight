{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9d0109e5e1049969f29465bf315b2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2e7fa5be5fa45709cfe33be7ad36929",
              "IPY_MODEL_b7ae9c4ca557420b9b2846f6f48d7c8c",
              "IPY_MODEL_26b8ddb9f93a40f98b0a76f523187a90"
            ],
            "layout": "IPY_MODEL_b470c56104fe43708dfe1e6b05271ac8"
          }
        },
        "d2e7fa5be5fa45709cfe33be7ad36929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a8a40a233564e858b51ccf4aeb31956",
            "placeholder": "​",
            "style": "IPY_MODEL_20bbcca4f821477fb63240621565dca7",
            "value": "modules.json: 100%"
          }
        },
        "b7ae9c4ca557420b9b2846f6f48d7c8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_940354acc23a42feaae57908c8bbdc8b",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea402b932e8240fba3d50b4b39b78882",
            "value": 349
          }
        },
        "26b8ddb9f93a40f98b0a76f523187a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b73971bed6b04e6a86a98fca40e57abd",
            "placeholder": "​",
            "style": "IPY_MODEL_a7bc565088304370863ac21e42aae2fe",
            "value": " 349/349 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "b470c56104fe43708dfe1e6b05271ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a8a40a233564e858b51ccf4aeb31956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20bbcca4f821477fb63240621565dca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "940354acc23a42feaae57908c8bbdc8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea402b932e8240fba3d50b4b39b78882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b73971bed6b04e6a86a98fca40e57abd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7bc565088304370863ac21e42aae2fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52060ecb7616484f98a98eee02654c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdee23b5b7c441c181dc09f89a70a82c",
              "IPY_MODEL_e9300288564e48ccb529487898c75d8d",
              "IPY_MODEL_4e6a49016cdd4d37a35216827f21055a"
            ],
            "layout": "IPY_MODEL_3f209616ebe04becbdb604685b3be8bb"
          }
        },
        "cdee23b5b7c441c181dc09f89a70a82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_118b8da53bc24c94bc054912b25db9ed",
            "placeholder": "​",
            "style": "IPY_MODEL_0f344c97b41a458192c7665e00964d80",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "e9300288564e48ccb529487898c75d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_777129fb06434ce4ba12833eb704a504",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e44e893f14044209992c773d1da04a8",
            "value": 116
          }
        },
        "4e6a49016cdd4d37a35216827f21055a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d7751beca1a4283bfc54210801cecf5",
            "placeholder": "​",
            "style": "IPY_MODEL_c851825efe104b689af3faf0c887a405",
            "value": " 116/116 [00:00&lt;00:00, 2.32kB/s]"
          }
        },
        "3f209616ebe04becbdb604685b3be8bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118b8da53bc24c94bc054912b25db9ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f344c97b41a458192c7665e00964d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "777129fb06434ce4ba12833eb704a504": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e44e893f14044209992c773d1da04a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d7751beca1a4283bfc54210801cecf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c851825efe104b689af3faf0c887a405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "306905a9030949a1914d5925fd1591f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff4ed499a66540a197a39f9aa2cbf5e6",
              "IPY_MODEL_dea449ab363c48c4b8ab80c32d6b04be",
              "IPY_MODEL_624d5cb2e0c547f2a469366e43bc54d1"
            ],
            "layout": "IPY_MODEL_ba05679df38c4f6786fcae866fd31d17"
          }
        },
        "ff4ed499a66540a197a39f9aa2cbf5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a460dd85724a4125adc45bb886d29e51",
            "placeholder": "​",
            "style": "IPY_MODEL_64a5a4e581cf4c1dbfbf67302996e959",
            "value": "README.md: 100%"
          }
        },
        "dea449ab363c48c4b8ab80c32d6b04be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91b9f270f4c646f6bf0c84be596af9ab",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef7c31bdacdd4b2c979b9445e8b4222f",
            "value": 10659
          }
        },
        "624d5cb2e0c547f2a469366e43bc54d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb9434cdcfc24591a238059b91a52e1d",
            "placeholder": "​",
            "style": "IPY_MODEL_f2c15fa21e7f42e596875c37c939b327",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 106kB/s]"
          }
        },
        "ba05679df38c4f6786fcae866fd31d17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a460dd85724a4125adc45bb886d29e51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64a5a4e581cf4c1dbfbf67302996e959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91b9f270f4c646f6bf0c84be596af9ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef7c31bdacdd4b2c979b9445e8b4222f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb9434cdcfc24591a238059b91a52e1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c15fa21e7f42e596875c37c939b327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "574c7a633ae4482ea8f069e2a6b80cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d9ae32bf3624b29b3620b5c3e33b793",
              "IPY_MODEL_af634cf8b20b4b889e64ad4823e77216",
              "IPY_MODEL_d92854a6f45742c1a8c27811cbc0e8d9"
            ],
            "layout": "IPY_MODEL_44182162aefc4f22a0cdaa31752fc205"
          }
        },
        "9d9ae32bf3624b29b3620b5c3e33b793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d78f253923483a8683f0edb13a9331",
            "placeholder": "​",
            "style": "IPY_MODEL_43c450f47660420fbf33a3de832d752e",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "af634cf8b20b4b889e64ad4823e77216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb8b0565175b436da52298eccfd708a6",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4430cc80b73646568c3b3192011d218b",
            "value": 53
          }
        },
        "d92854a6f45742c1a8c27811cbc0e8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa058cc58a34d03992260ecc28ea6f0",
            "placeholder": "​",
            "style": "IPY_MODEL_2d8dc385f47e4d9c9fe1d5333e089b9e",
            "value": " 53.0/53.0 [00:00&lt;00:00, 855B/s]"
          }
        },
        "44182162aefc4f22a0cdaa31752fc205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d78f253923483a8683f0edb13a9331": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c450f47660420fbf33a3de832d752e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb8b0565175b436da52298eccfd708a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4430cc80b73646568c3b3192011d218b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfa058cc58a34d03992260ecc28ea6f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d8dc385f47e4d9c9fe1d5333e089b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c3b5b98750b464c888c3a762f9ca29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6387ee6dfcb4d46b5ac99355376f6d8",
              "IPY_MODEL_0a36c0f6f2bb47e5bc81957fd2c73d8a",
              "IPY_MODEL_76470c67a59346989e6c07a3765f7d2e"
            ],
            "layout": "IPY_MODEL_fe9320b1272e4f61886eef15a32fbe37"
          }
        },
        "e6387ee6dfcb4d46b5ac99355376f6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b65955e652114220990c9351868e86e0",
            "placeholder": "​",
            "style": "IPY_MODEL_f4e9e692313e4b4d8ad8d2f2ff74afed",
            "value": "config.json: 100%"
          }
        },
        "0a36c0f6f2bb47e5bc81957fd2c73d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5958c3eca704d60aaf08099ae83bfc3",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_938957e64cc94eefb25e84fdb73ced30",
            "value": 612
          }
        },
        "76470c67a59346989e6c07a3765f7d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a037907135e645a3a799af0dc3305a1e",
            "placeholder": "​",
            "style": "IPY_MODEL_66cef9b92f944335afd83e92c940181a",
            "value": " 612/612 [00:00&lt;00:00, 14.8kB/s]"
          }
        },
        "fe9320b1272e4f61886eef15a32fbe37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b65955e652114220990c9351868e86e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4e9e692313e4b4d8ad8d2f2ff74afed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5958c3eca704d60aaf08099ae83bfc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "938957e64cc94eefb25e84fdb73ced30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a037907135e645a3a799af0dc3305a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66cef9b92f944335afd83e92c940181a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b60ddcdf4624d77a28f54d5cd0453bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c571a37d20c043db8ef5768f27dfd331",
              "IPY_MODEL_39f04184402246059e172c0a23dc8cdb",
              "IPY_MODEL_a7c1c874c2f54a5181aa06d330d0454b"
            ],
            "layout": "IPY_MODEL_aa884018ce51432db2d23f7ba92a1f40"
          }
        },
        "c571a37d20c043db8ef5768f27dfd331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d45b15a91b034b4cb951bf6d556651ec",
            "placeholder": "​",
            "style": "IPY_MODEL_5f649e18866d44de9847a7140e07d4d4",
            "value": "model.safetensors: 100%"
          }
        },
        "39f04184402246059e172c0a23dc8cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38549d6d90b74a2c9a73c648162467b0",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75d385125b514c1ca701dab5a3365348",
            "value": 90868376
          }
        },
        "a7c1c874c2f54a5181aa06d330d0454b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f84f2ea37924b2c96028c842ffbf6f0",
            "placeholder": "​",
            "style": "IPY_MODEL_839e3b3d3504462cbfb6a989c4638054",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 126MB/s]"
          }
        },
        "aa884018ce51432db2d23f7ba92a1f40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d45b15a91b034b4cb951bf6d556651ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f649e18866d44de9847a7140e07d4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38549d6d90b74a2c9a73c648162467b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d385125b514c1ca701dab5a3365348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f84f2ea37924b2c96028c842ffbf6f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "839e3b3d3504462cbfb6a989c4638054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48726e26a5044024924dfc5c6f89490d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce7e4faebe3e408fabb2d488b964145e",
              "IPY_MODEL_b80f8713a3074bec9359750c1d47d715",
              "IPY_MODEL_0402259f438c4a848fad4874f9bb2123"
            ],
            "layout": "IPY_MODEL_1b0a44229df64dcf8f860df2159be5d6"
          }
        },
        "ce7e4faebe3e408fabb2d488b964145e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32cee06de61e4f12ba6960589502963b",
            "placeholder": "​",
            "style": "IPY_MODEL_095055f2550a46f3aa2a89d7fcfe67a9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b80f8713a3074bec9359750c1d47d715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_800d6db722fd46ee8cdb60843aeb89a6",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_663ada38bd124d1f860d39f254017cf6",
            "value": 350
          }
        },
        "0402259f438c4a848fad4874f9bb2123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_094125e2a62a43e69097599418f6e1bf",
            "placeholder": "​",
            "style": "IPY_MODEL_39d6231f5152429d8ef523dcb7281f8b",
            "value": " 350/350 [00:00&lt;00:00, 4.69kB/s]"
          }
        },
        "1b0a44229df64dcf8f860df2159be5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32cee06de61e4f12ba6960589502963b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "095055f2550a46f3aa2a89d7fcfe67a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "800d6db722fd46ee8cdb60843aeb89a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "663ada38bd124d1f860d39f254017cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "094125e2a62a43e69097599418f6e1bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d6231f5152429d8ef523dcb7281f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7a3f1d04e914cc590a3b244ad050d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9876342354c74680949d90242cbdd9d2",
              "IPY_MODEL_fb453db8c80b45a881691aef8c894bb5",
              "IPY_MODEL_0459c06808e54bc69761547e431af588"
            ],
            "layout": "IPY_MODEL_304f210bbae14497bc0656f2d319bb62"
          }
        },
        "9876342354c74680949d90242cbdd9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b62b32476d04320aa2e948ea230be05",
            "placeholder": "​",
            "style": "IPY_MODEL_ef1d4873d9b643e1996828d82728e834",
            "value": "vocab.txt: 100%"
          }
        },
        "fb453db8c80b45a881691aef8c894bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7545e622d10a47029b510e3602543d89",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e98ca65986fe4267a16c2897facb6283",
            "value": 231508
          }
        },
        "0459c06808e54bc69761547e431af588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c0ca43808ee417c84ae5f20912d74f9",
            "placeholder": "​",
            "style": "IPY_MODEL_36b8b50df1a54166a39edeb79369f3bd",
            "value": " 232k/232k [00:00&lt;00:00, 3.30MB/s]"
          }
        },
        "304f210bbae14497bc0656f2d319bb62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b62b32476d04320aa2e948ea230be05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef1d4873d9b643e1996828d82728e834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7545e622d10a47029b510e3602543d89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98ca65986fe4267a16c2897facb6283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c0ca43808ee417c84ae5f20912d74f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36b8b50df1a54166a39edeb79369f3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f8d1f9e8b8b462186034e7463c15d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcd9e5711efd4e6ba0b4c3eacb26cc99",
              "IPY_MODEL_b9137278926f476c8971519b6c887967",
              "IPY_MODEL_b723b2ab1c0847e286cccf78656a011c"
            ],
            "layout": "IPY_MODEL_f6dd2049c5d24ed5aa4d09d40299c93a"
          }
        },
        "bcd9e5711efd4e6ba0b4c3eacb26cc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a1c3b84deda4234b65c44d52d45c0f1",
            "placeholder": "​",
            "style": "IPY_MODEL_4c92c40026bf4085b93519ca539ed5df",
            "value": "tokenizer.json: 100%"
          }
        },
        "b9137278926f476c8971519b6c887967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a32ae137a1042f09e39ff3caf57367d",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20228601d1664e708d2a25697ddd4f20",
            "value": 466247
          }
        },
        "b723b2ab1c0847e286cccf78656a011c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d291ff844b44fadaae68b669032165b",
            "placeholder": "​",
            "style": "IPY_MODEL_37d2a2924439424aa0acc228d6a9e199",
            "value": " 466k/466k [00:00&lt;00:00, 5.59MB/s]"
          }
        },
        "f6dd2049c5d24ed5aa4d09d40299c93a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a1c3b84deda4234b65c44d52d45c0f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c92c40026bf4085b93519ca539ed5df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a32ae137a1042f09e39ff3caf57367d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20228601d1664e708d2a25697ddd4f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d291ff844b44fadaae68b669032165b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d2a2924439424aa0acc228d6a9e199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc23330ba59b4bb18c43d04df38d3bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33e3a975405f4a769b32a968b0c2067e",
              "IPY_MODEL_295d5b15506b424cbd1e658553fd1a9d",
              "IPY_MODEL_7510634d22d7427b94d92995c3c71f4e"
            ],
            "layout": "IPY_MODEL_24346f14de814d0aaa3017c6f6196293"
          }
        },
        "33e3a975405f4a769b32a968b0c2067e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6abd167f71054b9682ed9a25dab77d1e",
            "placeholder": "​",
            "style": "IPY_MODEL_122280c127a74962ad591cf41d513762",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "295d5b15506b424cbd1e658553fd1a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf8af3b2fb2543a58a72b22ab960b0c5",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faa72f65c7994aa8a0806fd53c08bb8b",
            "value": 112
          }
        },
        "7510634d22d7427b94d92995c3c71f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6258d5e779844d3281ed39adc82414a3",
            "placeholder": "​",
            "style": "IPY_MODEL_62b12b0de97540eba09e020091148126",
            "value": " 112/112 [00:00&lt;00:00, 7.00kB/s]"
          }
        },
        "24346f14de814d0aaa3017c6f6196293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6abd167f71054b9682ed9a25dab77d1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122280c127a74962ad591cf41d513762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf8af3b2fb2543a58a72b22ab960b0c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faa72f65c7994aa8a0806fd53c08bb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6258d5e779844d3281ed39adc82414a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62b12b0de97540eba09e020091148126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "976978f18b84423cbe9deee6f6b5e9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_896b811b87684b5f93b107fb3977a877",
              "IPY_MODEL_41cbdb31cc2d4f9ba2b3dfb421aba723",
              "IPY_MODEL_028d4687b6b248bbafdf71c5eab3bfe6"
            ],
            "layout": "IPY_MODEL_03891c990b214e1ab8a345d251f6d7d0"
          }
        },
        "896b811b87684b5f93b107fb3977a877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca7a6768524047aebe038856bbff6c03",
            "placeholder": "​",
            "style": "IPY_MODEL_9abe80d0c53144b1b09ebf065523c081",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "41cbdb31cc2d4f9ba2b3dfb421aba723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_112bedeec62e4811a9e26f1f3aec07a5",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3e9b7d94dd84c69af1af862f86c116b",
            "value": 190
          }
        },
        "028d4687b6b248bbafdf71c5eab3bfe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e990f2330b4449a08fcb0e8f6905b901",
            "placeholder": "​",
            "style": "IPY_MODEL_6c85b13cb718492093af6d511f17544c",
            "value": " 190/190 [00:00&lt;00:00, 6.86kB/s]"
          }
        },
        "03891c990b214e1ab8a345d251f6d7d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7a6768524047aebe038856bbff6c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9abe80d0c53144b1b09ebf065523c081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "112bedeec62e4811a9e26f1f3aec07a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e9b7d94dd84c69af1af862f86c116b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e990f2330b4449a08fcb0e8f6905b901": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c85b13cb718492093af6d511f17544c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the normal llm vs along with rag"
      ],
      "metadata": {
        "id": "ttHMR3_z9Dir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install langchain-pinecone\n",
        "!pip install sentence-transformers\n",
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jLbq_WPj0UJ",
        "outputId": "f064e687-6227-4e58-81f6-ce4878494e04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.12)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (3.10.10)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.2.2)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.12)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.6.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.15.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Requirement already satisfied: langchain-pinecone in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: aiohttp<3.10,>=3.9.5 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone) (3.9.5)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone) (0.3.12)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone) (1.26.4)\n",
            "Requirement already satisfied: pinecone-client<6.0.0,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone) (5.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (1.15.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (4.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (4.12.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (4.66.5)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (2.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-pinecone) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (3.10.10)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-pinecone) (2.23.4)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<3.10,>=3.9.5->langchain-pinecone) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<3.10,>=3.9.5->langchain-pinecone) (0.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (3.4.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up questions to evaluate"
      ],
      "metadata": {
        "id": "VH3CDWMY-EWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Can you explain what a Bidirectional LSTM layer is in Keras?\",\n",
        "    \"How do I use the Keras Sequential model for building neural networks?\",\n",
        "    \"What are the key differences between the Dense and Conv2D layers in Keras?\",\n",
        "    \"How do you implement dropout in a Keras model to prevent overfitting?\",\n",
        "    \"Explain me why and how to use the EarlyStopping callback in Keras\",\n",
        "    \"Explain me how to build a LSTM model in Keras\",\n",
        "    \"How to use the ResNet model from keras applications for image classification?\",\n",
        "    \"Explain me the parameters used in the Conv2D layer in Keras\",\n",
        "    \"Explain me about the MNIST digits classification dataset\",\n",
        "    \"What is the purpose of using the Embedding layer in Keras?\",\n",
        "    \"Give me code for loading images using ImageDataGenerator in Keras\",\n",
        "    \"How do you perform transfer learning using a pre-trained model in Keras?\",\n",
        "    \"Can you explain how to save and load a Keras model?\",\n",
        "    \"How do I install Keras on my system?\",\n",
        "    \"What are the key features of Keras?\",\n",
        "    \"What is a Keras model and how do I create one?\",\n",
        "    \"How do I add a Dense layer to a Keras model?\",\n",
        "    \"What is the purpose of the Dropout layer in Keras?\",\n",
        "    \"How do I create a custom layer in Keras?\",\n",
        "    \"What is the use of Conv2D in Keras for building CNNs?\",\n",
        "    \"How do I use LSTM layers in a Sequential model?\",\n",
        "    \"How do I compile a Keras model?\",\n",
        "    \"How do I implement early stopping during model training in Keras?\",\n",
        "    \"Give me a code example of creating a simple Keras neural network.\",\n",
        "    \"Show me how to create a CNN using Keras.\",\n",
        "    \"How can I implement an RNN using Keras?\",\n",
        "    \"Provide code for training a Keras model with a custom dataset.\",\n",
        "    \"How can I use transfer learning with Keras?\",\n",
        "    \"What is the difference between model.fit() and model.fit_generator() in Keras?\",\n",
        "    \"What optimizers can I use in Keras, and which one is best for my problem?\",\n",
        "    \"How do I preprocess image data for training with Keras?\",\n",
        "    \"How can I use data augmentation in Keras to improve model performance?\",\n",
        "    \"Show me a code example of using Keras ImageDataGenerator.\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "TtZ3SueTgxZ8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gemini and vectorStore Setup"
      ],
      "metadata": {
        "id": "IFHqGUCD-ReR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up api keys\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "pinecone_api_key = userdata.get('pinecone_api_key')\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "groq_api_key = userdata.get('groq_api_key')\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "5ciO_4yLiZir"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import time\n",
        "# from config import VECTOR_DIMENSION\n",
        "VECTOR_DIMENSION = 384 # 768 do if nomic embeddings using\n",
        "class PineconeManager:\n",
        "    def __init__(self, api_key: str, index_name: str):\n",
        "        self.pc = Pinecone(api_key=api_key)\n",
        "        self.index_name = index_name\n",
        "        self.index = None\n",
        "        self.initialize_index() #to initialize the index\n",
        "    def initialize_index(self):\n",
        "        if self.index_name not in self.pc.list_indexes().names():#shd use list indexes here\n",
        "            print(f\"Creating index: {self.index_name}\")\n",
        "            self.pc.create_index(\n",
        "                name=self.index_name,\n",
        "                dimension=VECTOR_DIMENSION,\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\"\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            print(f\"Index {self.index_name} already exists\")\n",
        "\n",
        "        while not self.pc.describe_index(self.index_name).status['ready']:\n",
        "            time.sleep(1)\n",
        "\n",
        "        self.index = self.pc.Index(self.index_name)\n"
      ],
      "metadata": {
        "id": "23Xc754mjsbh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "INDEX_NAME = 'dlprojectchecknomic'\n",
        "pinecone_manager = PineconeManager(pinecone_api_key, INDEX_NAME)\n",
        "pinecone_manager.initialize_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlskmiPDjR-D",
        "outputId": "23bd3c7f-c187-43d2-fcba-7c12fabd6e72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index dlprojectchecknomic already exists\n",
            "Index dlprojectchecknomic already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_manager.index, embedding=embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "a9d0109e5e1049969f29465bf315b2cd",
            "d2e7fa5be5fa45709cfe33be7ad36929",
            "b7ae9c4ca557420b9b2846f6f48d7c8c",
            "26b8ddb9f93a40f98b0a76f523187a90",
            "b470c56104fe43708dfe1e6b05271ac8",
            "1a8a40a233564e858b51ccf4aeb31956",
            "20bbcca4f821477fb63240621565dca7",
            "940354acc23a42feaae57908c8bbdc8b",
            "ea402b932e8240fba3d50b4b39b78882",
            "b73971bed6b04e6a86a98fca40e57abd",
            "a7bc565088304370863ac21e42aae2fe",
            "52060ecb7616484f98a98eee02654c0f",
            "cdee23b5b7c441c181dc09f89a70a82c",
            "e9300288564e48ccb529487898c75d8d",
            "4e6a49016cdd4d37a35216827f21055a",
            "3f209616ebe04becbdb604685b3be8bb",
            "118b8da53bc24c94bc054912b25db9ed",
            "0f344c97b41a458192c7665e00964d80",
            "777129fb06434ce4ba12833eb704a504",
            "9e44e893f14044209992c773d1da04a8",
            "4d7751beca1a4283bfc54210801cecf5",
            "c851825efe104b689af3faf0c887a405",
            "306905a9030949a1914d5925fd1591f7",
            "ff4ed499a66540a197a39f9aa2cbf5e6",
            "dea449ab363c48c4b8ab80c32d6b04be",
            "624d5cb2e0c547f2a469366e43bc54d1",
            "ba05679df38c4f6786fcae866fd31d17",
            "a460dd85724a4125adc45bb886d29e51",
            "64a5a4e581cf4c1dbfbf67302996e959",
            "91b9f270f4c646f6bf0c84be596af9ab",
            "ef7c31bdacdd4b2c979b9445e8b4222f",
            "cb9434cdcfc24591a238059b91a52e1d",
            "f2c15fa21e7f42e596875c37c939b327",
            "574c7a633ae4482ea8f069e2a6b80cc6",
            "9d9ae32bf3624b29b3620b5c3e33b793",
            "af634cf8b20b4b889e64ad4823e77216",
            "d92854a6f45742c1a8c27811cbc0e8d9",
            "44182162aefc4f22a0cdaa31752fc205",
            "18d78f253923483a8683f0edb13a9331",
            "43c450f47660420fbf33a3de832d752e",
            "bb8b0565175b436da52298eccfd708a6",
            "4430cc80b73646568c3b3192011d218b",
            "cfa058cc58a34d03992260ecc28ea6f0",
            "2d8dc385f47e4d9c9fe1d5333e089b9e",
            "0c3b5b98750b464c888c3a762f9ca29e",
            "e6387ee6dfcb4d46b5ac99355376f6d8",
            "0a36c0f6f2bb47e5bc81957fd2c73d8a",
            "76470c67a59346989e6c07a3765f7d2e",
            "fe9320b1272e4f61886eef15a32fbe37",
            "b65955e652114220990c9351868e86e0",
            "f4e9e692313e4b4d8ad8d2f2ff74afed",
            "a5958c3eca704d60aaf08099ae83bfc3",
            "938957e64cc94eefb25e84fdb73ced30",
            "a037907135e645a3a799af0dc3305a1e",
            "66cef9b92f944335afd83e92c940181a",
            "8b60ddcdf4624d77a28f54d5cd0453bc",
            "c571a37d20c043db8ef5768f27dfd331",
            "39f04184402246059e172c0a23dc8cdb",
            "a7c1c874c2f54a5181aa06d330d0454b",
            "aa884018ce51432db2d23f7ba92a1f40",
            "d45b15a91b034b4cb951bf6d556651ec",
            "5f649e18866d44de9847a7140e07d4d4",
            "38549d6d90b74a2c9a73c648162467b0",
            "75d385125b514c1ca701dab5a3365348",
            "6f84f2ea37924b2c96028c842ffbf6f0",
            "839e3b3d3504462cbfb6a989c4638054",
            "48726e26a5044024924dfc5c6f89490d",
            "ce7e4faebe3e408fabb2d488b964145e",
            "b80f8713a3074bec9359750c1d47d715",
            "0402259f438c4a848fad4874f9bb2123",
            "1b0a44229df64dcf8f860df2159be5d6",
            "32cee06de61e4f12ba6960589502963b",
            "095055f2550a46f3aa2a89d7fcfe67a9",
            "800d6db722fd46ee8cdb60843aeb89a6",
            "663ada38bd124d1f860d39f254017cf6",
            "094125e2a62a43e69097599418f6e1bf",
            "39d6231f5152429d8ef523dcb7281f8b",
            "d7a3f1d04e914cc590a3b244ad050d02",
            "9876342354c74680949d90242cbdd9d2",
            "fb453db8c80b45a881691aef8c894bb5",
            "0459c06808e54bc69761547e431af588",
            "304f210bbae14497bc0656f2d319bb62",
            "3b62b32476d04320aa2e948ea230be05",
            "ef1d4873d9b643e1996828d82728e834",
            "7545e622d10a47029b510e3602543d89",
            "e98ca65986fe4267a16c2897facb6283",
            "5c0ca43808ee417c84ae5f20912d74f9",
            "36b8b50df1a54166a39edeb79369f3bd",
            "6f8d1f9e8b8b462186034e7463c15d3e",
            "bcd9e5711efd4e6ba0b4c3eacb26cc99",
            "b9137278926f476c8971519b6c887967",
            "b723b2ab1c0847e286cccf78656a011c",
            "f6dd2049c5d24ed5aa4d09d40299c93a",
            "8a1c3b84deda4234b65c44d52d45c0f1",
            "4c92c40026bf4085b93519ca539ed5df",
            "2a32ae137a1042f09e39ff3caf57367d",
            "20228601d1664e708d2a25697ddd4f20",
            "0d291ff844b44fadaae68b669032165b",
            "37d2a2924439424aa0acc228d6a9e199",
            "fc23330ba59b4bb18c43d04df38d3bd6",
            "33e3a975405f4a769b32a968b0c2067e",
            "295d5b15506b424cbd1e658553fd1a9d",
            "7510634d22d7427b94d92995c3c71f4e",
            "24346f14de814d0aaa3017c6f6196293",
            "6abd167f71054b9682ed9a25dab77d1e",
            "122280c127a74962ad591cf41d513762",
            "cf8af3b2fb2543a58a72b22ab960b0c5",
            "faa72f65c7994aa8a0806fd53c08bb8b",
            "6258d5e779844d3281ed39adc82414a3",
            "62b12b0de97540eba09e020091148126",
            "976978f18b84423cbe9deee6f6b5e9c7",
            "896b811b87684b5f93b107fb3977a877",
            "41cbdb31cc2d4f9ba2b3dfb421aba723",
            "028d4687b6b248bbafdf71c5eab3bfe6",
            "03891c990b214e1ab8a345d251f6d7d0",
            "ca7a6768524047aebe038856bbff6c03",
            "9abe80d0c53144b1b09ebf065523c081",
            "112bedeec62e4811a9e26f1f3aec07a5",
            "b3e9b7d94dd84c69af1af862f86c116b",
            "e990f2330b4449a08fcb0e8f6905b901",
            "6c85b13cb718492093af6d511f17544c"
          ]
        },
        "id": "2ncYEa44kS0r",
        "outputId": "e0432178-4015-4268-bf02-0274f29214fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c37ffac4ebc9>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9d0109e5e1049969f29465bf315b2cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52060ecb7616484f98a98eee02654c0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "306905a9030949a1914d5925fd1591f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "574c7a633ae4482ea8f069e2a6b80cc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c3b5b98750b464c888c3a762f9ca29e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b60ddcdf4624d77a28f54d5cd0453bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48726e26a5044024924dfc5c6f89490d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7a3f1d04e914cc590a3b244ad050d02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f8d1f9e8b8b462186034e7463c15d3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc23330ba59b4bb18c43d04df38d3bd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "976978f18b84423cbe9deee6f6b5e9c7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation with using finetune model vs gemini and then verification by mistral"
      ],
      "metadata": {
        "id": "rheirlneATha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client = Groq(\n",
        "    api_key = groq_api_key\n",
        ")\n",
        "def groq_mixtral_answer_generate(prompt):\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=\"mixtral-8x7b-32768\",\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "X39OKbw-_PMZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main function to get and store the results\n",
        "import time\n",
        "from random import uniform\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "tuned_model = genai.get_tuned_model('tunedModels/finetuninggemmafordl1-xxcubsl6ftaf')\n",
        "fine_tuned_model = genai.GenerativeModel(model_name=tuned_model.name)\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "rag_prompt = \"\"\"You are an intelligent assistant designed to provide accurate and relevant information from Keras documentation.\n",
        "\n",
        "        Here is the retrieved context, which may contain both explanatory text and meaningful code snippets:\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Carefully analyze the above context, considering both the text and any provided code for clarity.\n",
        "\n",
        "        Now, review the user's query:\n",
        "\n",
        "        {question}\n",
        "\n",
        "        Generate a detailed response that accurately addresses the query using the provided context. If the context includes relevant code, incorporate it into your response. Ensure that your answer is both clear and grounded in the provided content.\n",
        "\n",
        "        Response:\n",
        "        \"\"\"\n",
        "\n",
        "best_answer_prompt = \"\"\"\n",
        "You are an expert in evaluating responses based on technical documentation, specifically Keras. Your task is to compare two responses generated for a user's query regarding Keras.\n",
        "\n",
        "Evaluate the two responses provided below using the following criteria:\n",
        "1. **Relevance to Keras Documentation (0 to 1):** How closely does the response align with information from Keras documentation?\n",
        "2. **Accuracy (0 to 1):** How correct is the response in addressing the user's query with factual information?\n",
        "3. **Clarity (0 to 1):** How clear and easy to understand is the response?\n",
        "\n",
        "Based on these criteria, calculate a total score for each response (maximum score of 3). Also, consider if the response uses any relevant code examples from Keras documentation.\n",
        "\n",
        "**RAG Response:**\n",
        "{rag_response}\n",
        "\n",
        "**Normal Model Response:**\n",
        "{normal_response}\n",
        "\n",
        "Now, analyze and score each response. Provide reasoning for the scores, specifically noting if the response used Keras-specific information or included irrelevant details.\n",
        "\n",
        "- **RAG Response Total Score (out of 3):**\n",
        "- **Normal Model Response Total Score (out of 3):**\n",
        "- **Better Response:** (Specify which one is better, RAG or Normal, and explain why)\n",
        "\n",
        "Provide detailed reasoning for your choice, highlighting which response was more grounded in the Keras documentation and which one provided a clearer, more accurate answer to the user's query.\n",
        "\n",
        "Reasoning:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_and_store_results(questions,delay_range=(3, 10)):\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for question in questions:\n",
        "        try:\n",
        "            #answer from the rag\n",
        "            print(f\"Processing question: {question} \\n\")\n",
        "\n",
        "            print(\"Retrieving context... \\n\")\n",
        "            context_from_pinecone = retriever.get_relevant_documents(question)\n",
        "            context = \"\\n\\n\".join(doc.page_content for doc in context_from_pinecone)\n",
        "\n",
        "            time.sleep(uniform(delay_range[0], delay_range[1]))\n",
        "\n",
        "            print(\"Generating rag response... \\n\")\n",
        "            rag_prompt_formatted = rag_prompt.format(context=context, question=question)\n",
        "            rag_result = fine_tuned_model.generate_content(rag_prompt_formatted)\n",
        "            rag_response = rag_result.text\n",
        "\n",
        "            time.sleep(uniform(delay_range[0], delay_range[1]))\n",
        "\n",
        "            print(\"Generating normal response... \\n\")\n",
        "            #getting answer from the normal model\n",
        "            normal_result = model.generate_content(question)\n",
        "            normal_response = normal_result.text\n",
        "\n",
        "            time.sleep(uniform(delay_range[0], delay_range[1]))\n",
        "\n",
        "            print(\"Verifying the best answer... \\n\")\n",
        "            #verifying the best answer\n",
        "            best_answer_prompt_formated = best_answer_prompt.format(rag_response=rag_response, normal_response=normal_response)\n",
        "            verify_response = groq_mixtral_answer_generate(best_answer_prompt_formated)\n",
        "\n",
        "            print(\"Done! for question : \" + question + \" appending results \\n\")\n",
        "            #appending results\n",
        "            results.append({\n",
        "                'question': question,\n",
        "                'context retrived': context,\n",
        "                'rag_response': rag_response,\n",
        "                'normal_response': normal_response,\n",
        "                'verify_response': verify_response\n",
        "            })\n",
        "\n",
        "            time.sleep(uniform(delay_range[0], delay_range[1]))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {question}\")\n",
        "            print(f\"Error message: {str(e)}\")\n",
        "            results.append(None)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "F9cxPvwAkd6w"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers = get_and_store_results(questions,delay_range=(3, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dztMiV8NtW5y",
        "outputId": "fed2ef8d-8bb1-4874-8b33-d58b5dfd87c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing question: Can you explain what a Bidirectional LSTM layer is in Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-9705df8e3370>:66: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  context_from_pinecone = retriever.get_relevant_documents(question)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Can you explain what a Bidirectional LSTM layer is in Keras? appending results \n",
            "\n",
            "Processing question: How do I use the Keras Sequential model for building neural networks? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do I use the Keras Sequential model for building neural networks? appending results \n",
            "\n",
            "Processing question: What are the key differences between the Dense and Conv2D layers in Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : What are the key differences between the Dense and Conv2D layers in Keras? appending results \n",
            "\n",
            "Processing question: How do you implement dropout in a Keras model to prevent overfitting? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do you implement dropout in a Keras model to prevent overfitting? appending results \n",
            "\n",
            "Processing question: Explain me why and how to use the EarlyStopping callback in Keras \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Explain me why and how to use the EarlyStopping callback in Keras appending results \n",
            "\n",
            "Processing question: Explain me how to build a LSTM model in Keras \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Explain me how to build a LSTM model in Keras appending results \n",
            "\n",
            "Processing question: How to use the ResNet model from keras applications for image classification? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How to use the ResNet model from keras applications for image classification? appending results \n",
            "\n",
            "Processing question: Explain me the parameters used in the Conv2D layer in Keras \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Explain me the parameters used in the Conv2D layer in Keras appending results \n",
            "\n",
            "Processing question: Explain me about the MNIST digits classification dataset \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Explain me about the MNIST digits classification dataset appending results \n",
            "\n",
            "Processing question: What is the purpose of using the Embedding layer in Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : What is the purpose of using the Embedding layer in Keras? appending results \n",
            "\n",
            "Processing question: Give me code for loading images using ImageDataGenerator in Keras \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Give me code for loading images using ImageDataGenerator in Keras appending results \n",
            "\n",
            "Processing question: How do you perform transfer learning using a pre-trained model in Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do you perform transfer learning using a pre-trained model in Keras? appending results \n",
            "\n",
            "Processing question: Can you explain how to save and load a Keras model? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Can you explain how to save and load a Keras model? appending results \n",
            "\n",
            "Processing question: How do I install Keras on my system? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do I install Keras on my system? appending results \n",
            "\n",
            "Processing question: What are the key features of Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : What are the key features of Keras? appending results \n",
            "\n",
            "Processing question: What is a Keras model and how do I create one? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : What is a Keras model and how do I create one? appending results \n",
            "\n",
            "Processing question: How do I add a Dense layer to a Keras model? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do I add a Dense layer to a Keras model? appending results \n",
            "\n",
            "Processing question: What is the purpose of the Dropout layer in Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : What is the purpose of the Dropout layer in Keras? appending results \n",
            "\n",
            "Processing question: How do I create a custom layer in Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do I create a custom layer in Keras? appending results \n",
            "\n",
            "Processing question: What is the use of Conv2D in Keras for building CNNs? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : What is the use of Conv2D in Keras for building CNNs? appending results \n",
            "\n",
            "Processing question: How do I use LSTM layers in a Sequential model? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do I use LSTM layers in a Sequential model? appending results \n",
            "\n",
            "Processing question: How do I compile a Keras model? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do I compile a Keras model? appending results \n",
            "\n",
            "Processing question: How do I implement early stopping during model training in Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do I implement early stopping during model training in Keras? appending results \n",
            "\n",
            "Processing question: Give me a code example of creating a simple Keras neural network. \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Give me a code example of creating a simple Keras neural network. appending results \n",
            "\n",
            "Processing question: Show me how to create a CNN using Keras. \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Show me how to create a CNN using Keras. appending results \n",
            "\n",
            "Processing question: How can I implement an RNN using Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How can I implement an RNN using Keras? appending results \n",
            "\n",
            "Processing question: Provide code for training a Keras model with a custom dataset. \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Provide code for training a Keras model with a custom dataset. appending results \n",
            "\n",
            "Processing question: How can I use transfer learning with Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How can I use transfer learning with Keras? appending results \n",
            "\n",
            "Processing question: What is the difference between model.fit() and model.fit_generator() in Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : What is the difference between model.fit() and model.fit_generator() in Keras? appending results \n",
            "\n",
            "Processing question: What optimizers can I use in Keras, and which one is best for my problem? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : What optimizers can I use in Keras, and which one is best for my problem? appending results \n",
            "\n",
            "Processing question: How do I preprocess image data for training with Keras? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How do I preprocess image data for training with Keras? appending results \n",
            "\n",
            "Processing question: How can I use data augmentation in Keras to improve model performance? \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : How can I use data augmentation in Keras to improve model performance? appending results \n",
            "\n",
            "Processing question: Show me a code example of using Keras ImageDataGenerator. \n",
            "\n",
            "Retrieving context... \n",
            "\n",
            "Generating rag response... \n",
            "\n",
            "Generating normal response... \n",
            "\n",
            "Verifying the best answer... \n",
            "\n",
            "Done! for question : Show me a code example of using Keras ImageDataGenerator. appending results \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to filter out None values and save the results to a CSV file\n",
        "def save_results_to_csv(answers, file_name=\"results.csv\"):\n",
        "    # Filter out None values from the list\n",
        "    filtered_answers = [answer for answer in answers if answer is not None]\n",
        "\n",
        "    # Create a DataFrame from the filtered list of dictionaries\n",
        "    df = pd.DataFrame(filtered_answers)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(file_name, index=False)\n",
        "\n",
        "    print(f\"Results successfully saved to {file_name}\")\n",
        "\n",
        "# Call the function to save your results\n",
        "save_results_to_csv(answers, file_name=\"answers_results.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQhXwCGWtbqH",
        "outputId": "c8d6b97b-9713-4e35-c9ab-bbab787e501c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results successfully saved to answers_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('answers_results.csv')\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "snG9_N-ByKAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "e08ac831-c359-4a96-944f-86b941cbad2d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  Can you explain what a Bidirectional LSTM laye...   \n",
              "1  How do I use the Keras Sequential model for bu...   \n",
              "2  What are the key differences between the Dense...   \n",
              "3  How do you implement dropout in a Keras model ...   \n",
              "4  Explain me why and how to use the EarlyStoppin...   \n",
              "5      Explain me how to build a LSTM model in Keras   \n",
              "6  How to use the ResNet model from keras applica...   \n",
              "7  Explain me the parameters used in the Conv2D l...   \n",
              "8  Explain me about the MNIST digits classificati...   \n",
              "9  What is the purpose of using the Embedding lay...   \n",
              "\n",
              "                                    context retrived  \\\n",
              "0  3D Convolutional LSTM.\\nSimilar to an LSTM lay...   \n",
              "1  You can create a Sequential model by passing a...   \n",
              "2  2D transposed convolution layer.\\nThe need for...   \n",
              "3  Applies dropout to the input.\\nTheDropoutlayer...   \n",
              "4  Authors:Rick Chao, Francois CholletDate create...   \n",
              "5  There are three ways to create Keras models:\\n...   \n",
              "6  Instantiates the ResNet50V2 architecture.\\nRef...   \n",
              "7  2D Convolutional LSTM.\\nSimilar to an LSTM lay...   \n",
              "8  Loads the MNIST dataset.\\nThis is a dataset of...   \n",
              "9  In the mixed dimension embedding technique, we...   \n",
              "\n",
              "                                        rag_response  \\\n",
              "0  A Bidirectional LSTM layer in Keras is a speci...   \n",
              "1  The Keras Sequential model is a straightforwar...   \n",
              "2  The Dense layer is a fully connected layer, me...   \n",
              "3  To implement dropout in a Keras model, you can...   \n",
              "4  The EarlyStopping callback in Keras is used to...   \n",
              "5  To build an LSTM model in Keras, you can use t...   \n",
              "6  For image classification using the ResNet mode...   \n",
              "7  The Conv2D layer in Keras allows you to perfor...   \n",
              "8  The MNIST dataset is a collection of 70,000 gr...   \n",
              "9  The Embedding layer in Keras is used to conver...   \n",
              "\n",
              "                                     normal_response  \\\n",
              "0  **Bidirectional Long Short-Term Memory (BiLSTM...   \n",
              "1  **Importing the Necessary Libraries:**\\n\\n```p...   \n",
              "2  **Dense Layer (Fully Connected Layer):**\\n\\n* ...   \n",
              "3  ```python\\nimport tensorflow as tf\\n\\n# Create...   \n",
              "4  **Why Use EarlyStopping?**\\n\\nEarlyStopping is...   \n",
              "5  **Step 1: Import necessary libraries**\\n\\n```p...   \n",
              "6  **1. Import the necessary libraries:**\\n\\n```p...   \n",
              "7  **Parameters of the Conv2D Layer in Keras**\\n\\...   \n",
              "8  **MNIST (Modified National Institute of Standa...   \n",
              "9  The purpose of using the Embedding layer in Ke...   \n",
              "\n",
              "                                     verify_response  \n",
              "0  - **RAG Response Total Score (out of 3):**\\n\\n...  \n",
              "1  - **RAG Response Total Score (out of 3):**\\n\\n...  \n",
              "2  - **RAG Response Total Score (out of 3):**\\n\\n...  \n",
              "3  - **RAG Response Total Score (out of 3):**\\n\\n...  \n",
              "4  - **RAG Response Total Score (out of 3): 2.8**...  \n",
              "5  - **RAG Response Total Score (out of 3):**\\n\\n...  \n",
              "6  - **RAG Response Total Score (out of 3):**\\n\\n...  \n",
              "7  - **RAG Response Total Score (out of 3):**\\n\\n...  \n",
              "8  - **RAG Response Total Score (out of 3):**\\n\\n...  \n",
              "9  - **RAG Response Total Score (out of 3):**\\n\\n...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e51505ff-cd68-42bf-808e-21f3e346daf5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>context retrived</th>\n",
              "      <th>rag_response</th>\n",
              "      <th>normal_response</th>\n",
              "      <th>verify_response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can you explain what a Bidirectional LSTM laye...</td>\n",
              "      <td>3D Convolutional LSTM.\\nSimilar to an LSTM lay...</td>\n",
              "      <td>A Bidirectional LSTM layer in Keras is a speci...</td>\n",
              "      <td>**Bidirectional Long Short-Term Memory (BiLSTM...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3):**\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How do I use the Keras Sequential model for bu...</td>\n",
              "      <td>You can create a Sequential model by passing a...</td>\n",
              "      <td>The Keras Sequential model is a straightforwar...</td>\n",
              "      <td>**Importing the Necessary Libraries:**\\n\\n```p...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3):**\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are the key differences between the Dense...</td>\n",
              "      <td>2D transposed convolution layer.\\nThe need for...</td>\n",
              "      <td>The Dense layer is a fully connected layer, me...</td>\n",
              "      <td>**Dense Layer (Fully Connected Layer):**\\n\\n* ...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3):**\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do you implement dropout in a Keras model ...</td>\n",
              "      <td>Applies dropout to the input.\\nTheDropoutlayer...</td>\n",
              "      <td>To implement dropout in a Keras model, you can...</td>\n",
              "      <td>```python\\nimport tensorflow as tf\\n\\n# Create...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3):**\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Explain me why and how to use the EarlyStoppin...</td>\n",
              "      <td>Authors:Rick Chao, Francois CholletDate create...</td>\n",
              "      <td>The EarlyStopping callback in Keras is used to...</td>\n",
              "      <td>**Why Use EarlyStopping?**\\n\\nEarlyStopping is...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3): 2.8**...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Explain me how to build a LSTM model in Keras</td>\n",
              "      <td>There are three ways to create Keras models:\\n...</td>\n",
              "      <td>To build an LSTM model in Keras, you can use t...</td>\n",
              "      <td>**Step 1: Import necessary libraries**\\n\\n```p...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3):**\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How to use the ResNet model from keras applica...</td>\n",
              "      <td>Instantiates the ResNet50V2 architecture.\\nRef...</td>\n",
              "      <td>For image classification using the ResNet mode...</td>\n",
              "      <td>**1. Import the necessary libraries:**\\n\\n```p...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3):**\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Explain me the parameters used in the Conv2D l...</td>\n",
              "      <td>2D Convolutional LSTM.\\nSimilar to an LSTM lay...</td>\n",
              "      <td>The Conv2D layer in Keras allows you to perfor...</td>\n",
              "      <td>**Parameters of the Conv2D Layer in Keras**\\n\\...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3):**\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Explain me about the MNIST digits classificati...</td>\n",
              "      <td>Loads the MNIST dataset.\\nThis is a dataset of...</td>\n",
              "      <td>The MNIST dataset is a collection of 70,000 gr...</td>\n",
              "      <td>**MNIST (Modified National Institute of Standa...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3):**\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What is the purpose of using the Embedding lay...</td>\n",
              "      <td>In the mixed dimension embedding technique, we...</td>\n",
              "      <td>The Embedding layer in Keras is used to conver...</td>\n",
              "      <td>The purpose of using the Embedding layer in Ke...</td>\n",
              "      <td>- **RAG Response Total Score (out of 3):**\\n\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e51505ff-cd68-42bf-808e-21f3e346daf5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e51505ff-cd68-42bf-808e-21f3e346daf5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e51505ff-cd68-42bf-808e-21f3e346daf5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e4f6cbcd-c3d0-4140-acca-e8b16a6b529a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e4f6cbcd-c3d0-4140-acca-e8b16a6b529a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e4f6cbcd-c3d0-4140-acca-e8b16a6b529a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 33,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"How can I use data augmentation in Keras to improve model performance?\",\n          \"What is a Keras model and how do I create one?\",\n          \"Provide code for training a Keras model with a custom dataset.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context retrived\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"Our raw images have a variety of sizes. In addition, each pixel consists of 3 integer\\nvalues between 0 and 255 (RGB level values). This isn't a great fit for feeding a\\nneural network. We need to do 2 things:\\nIn general, it's a good practice to develop models that take raw data as input, as\\nopposed to models that take already-preprocessed data. The reason being that, if your\\nmodel expects preprocessed data, any time you export your model to use it elsewhere\\n(in a web browser, in a mobile app), you'll need to reimplement the exact same\\npreprocessing pipeline. This gets very tricky very quickly. So we should do the least\\n possible amount of preprocessing before hitting the model.\\nHere, we'll do image resizing in the data pipeline (because a deep neural network can\\nonly process contiguous batches of data), and we'll do the input value scaling as part\\nof the model, when we create it.\\nLet's resize images to 150x150:\\n\\n\\nCode:\\nresize_fn=keras.layers.Resizing(150,150)train_ds=train_ds.map(lambdax,y:(resize_fn(x),y))validation_ds=validation_ds.map(lambdax,y:(resize_fn(x),y))test_ds=test_ds.map(lambdax,y:(resize_fn(x),y))\\n\\n\\n\\nKeras data loading utilities, located inkeras_core.utils,\\nhelp you go from raw data on disk to atf.data.Datasetobject that can be\\nused to efficiently train a model.\\nThese loading utilites can be combined withpreprocessing layersto\\nfuther transform your input dataset before training.\\nHere's a quick example: let's say you have 10 folders, each containing\\n10,000 images from a different category, and you want to train a\\nclassifier that maps an image to its category.\\nYour training data folder would look like this:\\nYou may also have a validation data foldervalidation_data/structured in the\\nsame way.\\nYou could simply do:\\n\\n\\nCode:\\ntraining_data/\\n...class_a/\\n......a_image_1.jpg\\n......a_image_2.jpg\\n...class_b/\\n......b_image_1.jpg\\n......b_image_2.jpg\\netc.\\n\\nimportkeras_coreaskerastrain_ds=keras.utils.image_dataset_from_directory(directory='training_data/',labels='inferred',label_mode='categorical',batch_size=32,image_size=(256,256))validation_ds=keras.utils.image_dataset_from_directory(directory='validation_data/',labels='inferred',label_mode='categorical',batch_size=32,image_size=(256,256))model=keras.applications.Xception(weights=None,input_shape=(256,256,3),classes=10)model.compile(optimizer='rmsprop',loss='categorical_crossentropy')model.fit(train_ds,epochs=10,validation_data=validation_ds)\\n\\n\\n\\nKeras data loading utilities, located inkeras_core.utils,\\nhelp you go from raw data on disk to atf.data.Datasetobject that can be\\nused to efficiently train a model.\\nThese loading utilites can be combined withpreprocessing layersto\\nfuther transform your input dataset before training.\\nHere's a quick example: let's say you have 10 folders, each containing\\n10,000 images from a different category, and you want to train a\\nclassifier that maps an image to its category.\\nYour training data folder would look like this:\\nYou may also have a validation data foldervalidation_data/structured in the\\nsame way.\\nYou could simply do:\\n\\n\\nCode:\\ntraining_data/\\n...class_a/\\n......a_image_1.jpg\\n......a_image_2.jpg\\n...class_b/\\n......b_image_1.jpg\\n......b_image_2.jpg\\netc.\\n\\nimportkeras_coreaskerastrain_ds=keras.utils.image_dataset_from_directory(directory='training_data/',labels='inferred',label_mode='categorical',batch_size=32,image_size=(256,256))validation_ds=keras.utils.image_dataset_from_directory(directory='validation_data/',labels='inferred',label_mode='categorical',batch_size=32,image_size=(256,256))model=keras.applications.Xception(weights=None,input_shape=(256,256,3),classes=10)model.compile(optimizer='rmsprop',loss='categorical_crossentropy')model.fit(train_ds,epochs=10,validation_data=validation_ds)\\n\\n\",\n          \"There are three ways to create Keras models:\\n\\n\\nCode:\\n\\n\\nThere are three ways to create Keras models:\\n\\n\\nCode:\\n\\n\\nUsed to instantiate a Keras tensor.\\nA Keras tensor is a symbolic tensor-like object, which we augment with\\ncertain attributes that allow us to build a Keras model just by knowing the\\ninputs and outputs of the model.\\nFor instance, ifa,bandcare Keras tensors,\\nit becomes possible to do:model = Model(input=[a, b], output=c)\\nArguments\\nReturns\\nA Keras tensor.\\nExample\\n\\n\\nCode:\\nkeras_core.Input(shape=None,batch_size=None,dtype=None,batch_shape=None,name=None,tensor=None)\\n\\n# This is a logistic regression in Kerasx=Input(shape=(32,))y=Dense(16,activation='softmax')(x)model=Model(x,y)\\n\\n\",\n          \"There are three ways to create Keras models:\\n\\n\\nCode:\\n\\n\\nThere are three ways to create Keras models:\\n\\n\\nCode:\\n\\n\\nTo use Keras, will need to have the TensorFlow package installed.See detailed instructions.\\nOnce TensorFlow is installed, just import Keras via:\\nThe Keras codebase is also available on GitHub atkeras-team/keras.\\n\\n\\nCode:\\nfromtensorflowimportkeras\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"Data augmentation is a powerful technique to improve model performance by generating additional training data through random transformations. In Keras, you can use the `ImageDataGenerator` class for image augmentation. For example:\\n\\n        ```python\\n        train_datagen = keras.preprocessing.image.ImageDataGenerator(\\n            rescale=1./255,\\n            rotation_range=20,\\n            width_shift_range=0.2,\\n            height_shift_range=0.2,\\n            shear_range=0.2,\\n            zoom_range=0.2,\\n            horizontal_flip=True,\\n            fill_mode='nearest'\\n        )\\n        train_generator = train_datagen.flow_from_directory(\\n            'path/to/training/directory',\\n            target_size=(150,150),\\n            batch_size=32,\\n            class_mode='binary'\\n        )\\n        ```\\n        This code applies various transformations like rotations, shifts, and flips to augment the training data. You can then use this augmented data for training your model.\",\n          \"A Keras model is an object that groups layers and provides features for training and inference. You can create a Keras model using `Model(input=..., output=...)`, where \\\"input\\\" specifies the model's input tensor(s) and \\\"output\\\" specifies the model's output tensor(s). For example:\\n\\n        ```python\\n        inputs = keras.Input(shape=(32,))\\n        x = keras.Dense(16, activation=\\\"softmax\\\")(inputs)\\n        model = keras.Model(inputs, x)\\n        ```\\n        This creates a Keras model with an input shape of (32,) and an output shape of (16,).\",\n          \"The provided context does not include information on training a Keras model with a custom dataset. Therefore, I cannot generate a response to this query from the provided context.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"normal_response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"**Data Augmentation in Keras**\\n\\nData augmentation is a technique used to increase the size and diversity of your training data, helping improve model performance. Keras provides several data augmentation methods through its `ImageDataGenerator` class.\\n\\n**Steps to Use Data Augmentation:**\\n\\n1. **Create an `ImageDataGenerator` instance:**\\n```python\\nfrom keras.preprocessing.image import ImageDataGenerator\\n\\nimage_datagen = ImageDataGenerator(\\n    rescale=1./255,  # Scale pixel values to [0, 1]\\n    rotation_range=40,  # Rotate images by up to 40 degrees\\n    width_shift_range=0.2,  # Translate images horizontally by up to 20% of their width\\n    height_shift_range=0.2,  # Translate images vertically by up to 20% of their height\\n    shear_range=0.2,  # Shear images by up to 20%\\n    zoom_range=0.2,  # Zoom images by up to 20%\\n    horizontal_flip=True,  # Flip images horizontally\\n    vertical_flip=True,  # Flip images vertically\\n)\\n```\\n\\n2. **Apply augmentation to your training data:**\\n```python\\n# Generate augmented training data\\ntrain_generator = image_datagen.flow_from_directory(\\n    'train_dir/',  # Directory with training images\\n    target_size=(224, 224),  # Resize images to (224, 224)\\n    batch_size=32,  # Batch size\\n    class_mode='binary'  # Binary classification\\n)\\n```\\n\\n3. **Use the augmented data to train your model:**\\n```python\\nmodel = tf.keras.models.Sequential(...)  # Define your model\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\\n\\nmodel.fit(train_generator, epochs=10)\\n```\\n\\n**Additional Tips:**\\n\\n* Choose augmentation techniques that are appropriate for your data and task.\\n* Experiment with different augmentation parameters to find the optimal settings.\\n* Consider using a combination of techniques to maximize data diversity.\\n* Apply augmentation only to the training data, not the validation or test data.\\n* Use Keras's `RandomRotation`, `RandomFlip`, and `RandomCrop` layers for more fine-grained control over augmentation.\\n\\n**Benefits of Data Augmentation:**\\n\\n* Reduces overfitting by exposing the model to a wider range of data.\\n* Improves model robustness to noise and small variations in input data.\\n* Helps learn more generalizable features.\\n* Enhances model performance on unseen data.\",\n          \"**What is a Keras Model?**\\n\\nA Keras model is a computational graph that defines the architecture of a neural network. It consists of layers that process input data and generate output predictions. Layers can be stacked together to create complex neural networks.\\n\\n**How to Create a Keras Model:**\\n\\nThere are two main ways to create a Keras model:\\n\\n**1. Sequential API**\\n\\nThe Sequential API provides a simple and intuitive way to create models by stacking layers sequentially. Here's an example:\\n\\n```python\\nimport keras.layers as layers\\nimport keras.models as models\\n\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10,)))\\nmodel.add(layers.Dense(32, activation='relu'))\\nmodel.add(layers.Dense(5, activation='softmax'))\\n```\\n\\nThis model defines a feedforward neural network with:\\n- An input layer with 10 nodes\\n- Two hidden layers with 16 and 32 nodes, respectively\\n- An output layer with 5 nodes and a softmax activation\\n\\n**2. Functional API**\\n\\nThe Functional API provides more flexibility for creating complex models. It allows you to define layers independently and connect them manually. Here's an example:\\n\\n```python\\nfrom keras.layers import Input, Dense, Concatenate\\nfrom keras.models import Model\\n\\ninput_layer = Input(shape=(10,))\\nx = Dense(16, activation='relu')(input_layer)\\ny = Dense(32, activation='relu')(input_layer)\\noutput_layer = Concatenate()([x, y])\\n\\nmodel = Model(inputs=input_layer, outputs=output_layer)\\n```\\n\\nThis model defines a neural network with:\\n- An input layer with 10 nodes\\n- Two dense layers that share the same input\\n- A concatenation layer that combines the outputs of the dense layers\\n- No output activation function (you can add one if needed)\\n\\n**Additional Notes:**\\n\\n* You can use `summary()` on a model to print its architecture.\\n* You can compile a model using `compile()` to specify the loss function, optimizer, and metrics.\\n* You can train a model using `fit()` with training data and labels.\\n* You can evaluate a model using `evaluate()` with test data and labels.\",\n          \"```python\\nimport tensorflow as tf\\n\\n# Define the custom dataset\\ndataset = tf.data.Dataset.from_tensor_slices((\\n    tf.random.uniform((100, 10)),\\n    tf.random.uniform((100, 1))\\n))\\n\\n# Create the model\\nmodel = tf.keras.Sequential([\\n    tf.keras.layers.Dense(10, activation='relu'),\\n    tf.keras.layers.Dense(1, activation='sigmoid')\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\\n\\n# Train the model\\nmodel.fit(dataset, epochs=10)\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"verify_response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"- **RAG Response Total Score (out of 3):** 3\\n \\n  The RAG response is relevant to the Keras documentation, accurate in addressing the user's query with factual information, and clear in its explanation. It includes a concise and informative code example demonstrating data augmentation using `ImageDataGenerator` in Keras. The response is directly grounded in the Keras documentation.\\n\\n- **Normal Model Response Total Score (out of 3):** 2.8\\n \\n  The Normal model response is also relevant, accurate, and clear. It provides a more detailed explanation of data augmentation and its benefits. However, it does not include a code example, which slightly reduces its score. Nevertheless, it is directly grounded in the Keras documentation.\\n\\n- **Better Response:** Both responses are strong, but the RAG response will be considered better due to its inclusion of a code example. This makes it easier for users to apply the information and understand its practical implementation. Furthermore, users who prefer brevity might find the RAG response more appealing. However, under different circumstances, such as when more detailed information is required, the Normal model response would also be a good choice.\\n\\nIn conclusion, both responses are strong and well grounded in the Keras documentation. The RAG response is chosen as the better response because of its concise code example, making it easier for users to implement the information provided.\",\n          \"- **RAG Response Total Score (out of 3):**\\n\\n1. **Relevance to Keras Documentation (0.8):** The response provides a basic example of creating a Keras model using the Functional API which aligns with the Keras documentation, but it does not cover other important aspects such as the Sequential API or compilation and training of the model.\\n2. **Accuracy (1.0):** The response accurately describes how to create a simple Keras model using the Functional API.\\n3. **Clarity (1.0):** The response is clear and easy to understand.\\n*Total score for RAG Response: 2.8/3*\\n\\n- **Normal Model Response Total Score (out of 3):**\\n\\n1. **Relevance to Keras Documentation (1.0):** The response is well-grounded in the Keras documentation and covers various important aspects of Keras models, such as Sequential API, Functional API, compilation, and training.\\n2. **Accuracy (1.0):** The response accurately describes how to create a Keras model using both the Sequential and Functional APIs, and it also includes information on how to compile and train a model.\\n3. **Clarity (1.0):** The response is clear and easy to understand, providing detailed examples for each topic.\\n*Total score for Normal Model Response: 3.0/3*\\n\\n- **Better Response:** The Normal Model Response is better. It is more comprehensive and covers various important aspects of Keras models, making it more useful for users. Additionally, it provides examples of both Sequential and Functional APIs, which allows users to choose the one that fits their needs.\\n\\nReasoning: While both responses are accurate, the Normal Model Response is more comprehensive and covers various important aspects of Keras models. It explains the Sequential API, Functional API, and other processes like compiling and training a model which are essential for using Keras effectively. Furthermore, the Normal Model Response divides the explanation into two main approaches, providing clear examples for each topic; this makes it easier for users to understand and follow the provided instructions.\",\n          \"- **RAG Response Total Score (out of 3):**\\n\\n   **Relevance to Keras Documentation:** 0 (The response does not provide any information related to Keras documentation.)\\n\\n   **Accuracy:** Not applicable (The response does not address the user's query.)\\n\\n   **Clarity:** Not applicable (The response does not address the user's query.)\\n\\n   **Total Score:** 0 (The response does not provide any valuable information addressing the user's query.)\\n\\n- **Normal Model Response Total Score (out of 3):**\\n\\n   **Relevance to Keras Documentation:** 1 (The response demonstrates training a Keras model using a custom dataset, which is relevant to the user's query.)\\n\\n   **Accuracy:** 1 (The example provided is correct based on the given context.)\\n\\n   **Clarity:** 1 (The example code is clear and easy to understand.)\\n\\n   **Total Score:** 3 (The response provides a valuable demonstration of training a Keras model based on the user's query.)\\n\\n- **Better Response:** Normal Model Response\\n\\n   The Normal Model Response is more grounded in the Keras documentation as it directly demonstrates how to train a Keras model using a custom dataset. The response is clearer and more accurate, as it provides a working code example that addresses the user's query. On the other hand, the RAG response does not give any useful information related to the query, making the Normal Model Response the better option.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate over 10 rows and print the result\n",
        "i=0\n",
        "for index, row in df.iterrows():\n",
        "  i+=1\n",
        "  if i>10:\n",
        "    break\n",
        "  print(f\"Question: {row['question']} \\n\")\n",
        "  print(f\"Context Retrieved: {row['context retrived']} \\n\")\n",
        "  print(f\"RAG Response: {row['rag_response']} \\n\")\n",
        "  print(f\"Normal Response: {row['normal_response']} \\n\")\n",
        "  print(f\"Verify Response: {row['verify_response']} \\n\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TRWijdX52yE",
        "outputId": "8e43712b-c3e9-41c1-f074-6c4877adc770"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Can you explain what a Bidirectional LSTM layer is in Keras? \n",
            "\n",
            "Context Retrieved: 3D Convolutional LSTM.\n",
            "Similar to an LSTM layer, but the input transformations\n",
            "and recurrent transformations are both convolutional.\n",
            "Arguments\n",
            "Call arguments\n",
            "Input shape\n",
            "Output shape\n",
            "References\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.ConvLSTM3D(filters,kernel_size,strides=1,padding=\"valid\",data_format=None,dilation_rate=1,activation=\"tanh\",recurrent_activation=\"sigmoid\",use_bias=True,kernel_initializer=\"glorot_uniform\",recurrent_initializer=\"orthogonal\",bias_initializer=\"zeros\",unit_forget_bias=True,kernel_regularizer=None,recurrent_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,recurrent_constraint=None,bias_constraint=None,dropout=0.0,recurrent_dropout=0.0,seed=None,return_sequences=False,return_state=False,go_backwards=False,stateful=False,**kwargs)\n",
            "\n",
            "\n",
            "\n",
            "Bidirectional wrapper for RNNs.\n",
            "Arguments\n",
            "Call arguments\n",
            "The call arguments for this layer are the same as those of the\n",
            "wrapped RNN layer. Beware that when passing theinitial_stateargument during the call of this layer, the first half in the\n",
            "list of elements in theinitial_statelist will be passed to\n",
            "the forward RNN call and the last half in the list of elements\n",
            "will be passed to the backward RNN call.\n",
            "Note: instantiating aBidirectionallayer from an existing RNN layer\n",
            "instance will not reuse the weights state of the RNN layer instance -- theBidirectionallayer will have freshly initialized weights.\n",
            "Examples\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.Bidirectional(layer,merge_mode=\"concat\",weights=None,backward_layer=None,**kwargs)\n",
            "\n",
            "model=Sequential([Input(shape=(5,10)),Bidirectional(LSTM(10,return_sequences=True),Bidirectional(LSTM(10)),Dense(5,activation=\"softmax\"),])model.compile(loss='categorical_crossentropy',optimizer='rmsprop')# With custom backward layerforward_layer=LSTM(10,return_sequences=True)backward_layer=LSTM(10,activation='relu',return_sequences=True,go_backwards=True)model=Sequential([Input(shape=(5,10)),Bidirectional(forward_layer,backward_layer=backward_layer),Dense(5,activation=\"softmax\"),])model.compile(loss='categorical_crossentropy',optimizer='rmsprop')\n",
            "\n",
            "\n",
            "\n",
            "2D Convolutional LSTM.\n",
            "Similar to an LSTM layer, but the input transformations\n",
            "and recurrent transformations are both convolutional.\n",
            "Arguments\n",
            "Call arguments\n",
            "Input shape\n",
            "Output shape\n",
            "References\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.ConvLSTM2D(filters,kernel_size,strides=1,padding=\"valid\",data_format=None,dilation_rate=1,activation=\"tanh\",recurrent_activation=\"sigmoid\",use_bias=True,kernel_initializer=\"glorot_uniform\",recurrent_initializer=\"orthogonal\",bias_initializer=\"zeros\",unit_forget_bias=True,kernel_regularizer=None,recurrent_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,recurrent_constraint=None,bias_constraint=None,dropout=0.0,recurrent_dropout=0.0,seed=None,return_sequences=False,return_state=False,go_backwards=False,stateful=False,**kwargs)\n",
            "\n",
            " \n",
            "\n",
            "RAG Response: A Bidirectional LSTM layer in Keras is a special type of RNN layer that processes inputs in both forward and backward directions. This means that it effectively combines two separate LSTM layers, one running in the forward direction and the other running in the reverse direction. The outputs from these two LSTM layers are then combined according to the specified `merge_mode`. This allows the layer to capture temporal dependencies in both directions, potentially improving the model's ability to learn from sequential data.\n",
            "\n",
            "        Example code:\n",
            "        ```python\n",
            "        from keras.layers import Input, Bidirectional, LSTM, Dense\n",
            "        from keras.models import Sequential\n",
            "\n",
            "        model = Sequential([\n",
            "            Input(shape=(5, 10)),\n",
            "            Bidirectional(LSTM(10, return_sequences=True)),\n",
            "            Bidirectional(LSTM(10)),\n",
            "            Dense(5, activation=\"softmax\"),\n",
            "        ])\n",
            "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
            "        ```\n",
            "        In this example, the Bidirectional layer is used to wrap two LSTM layers, allowing for processing of sequences in both forward and backward directions. \n",
            "\n",
            "Normal Response: **Bidirectional Long Short-Term Memory (BiLSTM) Layer in Keras**\n",
            "\n",
            "A Bidirectional LSTM (BiLSTM) layer in Keras is a type of recurrent neural network (RNN) that processes sequential data both forwards and backwards, allowing it to capture long-term dependencies and context in both directions.\n",
            "\n",
            "**Architecture:**\n",
            "\n",
            "A BiLSTM layer consists of:\n",
            "\n",
            "* **Forward LSTM Cell:** Processes the input sequence from left to right.\n",
            "* **Backward LSTM Cell:** Processes the input sequence from right to left.\n",
            "* **Concatenation:** The outputs from the forward and backward cells are concatenated to produce the final output.\n",
            "\n",
            "**How it Works:**\n",
            "\n",
            "At each time step, the BiLSTM layer:\n",
            "\n",
            "1. **Updates the LSTM Memory Cells:** Each LSTM cell updates its memory based on the current input and the previous memory state.\n",
            "2. **Processes Forward and Backward:** The forward LSTM cell processes the input from left to right, while the backward LSTM cell processes the input from right to left.\n",
            "3. **Concatenates Outputs:** The outputs from both cells are concatenated together.\n",
            "\n",
            "This process is repeated for each time step in the input sequence, allowing the BiLSTM layer to learn dependencies and context both in the forward and backward directions.\n",
            "\n",
            "**Key Benefits:**\n",
            "\n",
            "* **Long-term Dependency Modeling:** Captures relationships between distant elements in the input sequence, even if they are separated by noise or irrelevant data.\n",
            "* **Contextual Understanding:** Provides a more comprehensive understanding of the context by processing data in both directions.\n",
            "* **Improved Sequence Modeling:** Enhances the ability to model time series, natural language, and other sequential data.\n",
            "\n",
            "**Usage in Keras:**\n",
            "\n",
            "To use a BiLSTM layer in Keras, you can use the following syntax:\n",
            "\n",
            "```python\n",
            "import keras\n",
            "\n",
            "inputs = keras.Input(shape=(timesteps, features))\n",
            "bilstm = keras.layers.Bidirectional(keras.layers.LSTM(units), merge_mode='concat')(inputs)\n",
            "```\n",
            "\n",
            "Where:\n",
            "\n",
            "* `timesteps` is the number of time steps in the input sequence.\n",
            "* `features` is the number of features in each time step.\n",
            "* `units` is the number of units in the LSTM cell.\n",
            "* `merge_mode` specifies how the outputs from the forward and backward cells are combined (options: 'concat', 'sum').\n",
            "\n",
            "**Applications:**\n",
            "\n",
            "BiLSTM layers are widely used in various deep learning tasks, including:\n",
            "\n",
            "* Natural Language Processing (NLP): Sequence classification, language modeling, machine translation.\n",
            "* Time Series Analysis: Stock market prediction, anomaly detection.\n",
            "* Speech Recognition: Feature extraction, language modeling.\n",
            "* Image Captioning: Generating descriptions of images or videos. \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3):**\n",
            "\n",
            "1. Relevance to Keras Documentation: 1\n",
            "   The response provides a good overview of a Bidirectional LSTM layer in Keras, but it lacks some specific details about the architecture and inner workings, which can be found in the Keras documentation.\n",
            "\n",
            "2. Accuracy: 1\n",
            "   The explanation of a Bidirectional LSTM layer in Keras is generally correct, but the description of `merge_mode` is not accurate. It combines the outputs by concatenation or summation, not by combining two separate LSTM layers as suggested in the response.\n",
            "\n",
            "3. Clarity: 1\n",
            "   The response is clear and easy to understand, but it could benefit from providing more specific details and code examples as they appear in the official documentation.\n",
            "\n",
            "   **RAG Response Total Score: 3/3**\n",
            "\n",
            "- **Normal Model Response Total Score (out of 3):**\n",
            "\n",
            "1. Relevance to Keras Documentation: 1\n",
            "   The response is mostly grounded in the Keras documentation, but it lacks an example of code usage, which could help users understand the implementation better.\n",
            "\n",
            "2. Accuracy: 1\n",
            "   The explanation of how a Bidirectional LSTM layer processes sequential data is mostly correct, but the section on `merge_mode` is not accurate. The outputs are combined through concatenation or summation, not by having a separate concatenation layer as implied in the response.\n",
            "\n",
            "3. Clarity: 1\n",
            "   The response is clear and easy to understand. It provides details about the architecture, inner workings, and benefits of using a Bidirectional LSTM layer.\n",
            "\n",
            "   **Normal Model Response Total Score: 3/3**\n",
            "\n",
            "- **Better Response:**\n",
            "\n",
            "It is difficult to definitively choose a better response, as both of them have their strengths and weaknesses. The RAG response provides a code example, which is helpful but lacks specifics regarding the architecture and inner workings. The Normal Model response offers more information about the architecture and how it works but does not include any code examples. Considering the user's query and the need for a more comprehensive answer, the **Normal Model response is slightly better**. It is more grounded in the Keras documentation and offers a clearer explanation of the BiLSTM layer and its benefits without relying on a code example. However, users might still benefit from a code example, so providing one alongside the Normal Model response would create a more complete and helpful answer.\n",
            "\n",
            "Please note that both responses could be improved by addressing their weaknesses and combining the strengths of each. A perfect response would be grounded in the Keras documentation, provide a clear explanation of the BiLSTM layer, and include a code example. \n",
            "\n",
            "\n",
            "\n",
            "Question: How do I use the Keras Sequential model for building neural networks? \n",
            "\n",
            "Context Retrieved: You can create a Sequential model by passing a list of layers to the Sequential\n",
            "constructor:\n",
            "Its layers are accessible via thelayersattribute:\n",
            "You can also create a Sequential model incrementally via theadd()method:\n",
            "Note that there's also a correspondingpop()method to remove layers:\n",
            "a Sequential model behaves very much like a list of layers.\n",
            "Also note that the Sequential constructor accepts anameargument, just like\n",
            "any layer or model in Keras. This is useful to annotate TensorBoard graphs\n",
            "with semantically meaningful names.\n",
            "\n",
            "\n",
            "Code:\n",
            "model=keras.Sequential([layers.Dense(2,activation=\"relu\"),layers.Dense(3,activation=\"relu\"),layers.Dense(4),])\n",
            "\n",
            "model.layers\n",
            "\n",
            "[<Dense name=dense, built=False>,\n",
            " <Dense name=dense_1, built=False>,\n",
            " <Dense name=dense_2, built=False>]\n",
            "\n",
            "model=keras.Sequential()model.add(layers.Dense(2,activation=\"relu\"))model.add(layers.Dense(3,activation=\"relu\"))model.add(layers.Dense(4))\n",
            "\n",
            "model.pop()print(len(model.layers))# 2\n",
            "\n",
            "2\n",
            "\n",
            "model=keras.Sequential(name=\"my_sequential\")model.add(layers.Dense(2,activation=\"relu\",name=\"layer1\"))model.add(layers.Dense(3,activation=\"relu\",name=\"layer2\"))model.add(layers.Dense(4,name=\"layer3\"))\n",
            "\n",
            "\n",
            "\n",
            "You can create a Sequential model by passing a list of layers to the Sequential\n",
            "constructor:\n",
            "Its layers are accessible via thelayersattribute:\n",
            "You can also create a Sequential model incrementally via theadd()method:\n",
            "Note that there's also a correspondingpop()method to remove layers:\n",
            "a Sequential model behaves very much like a list of layers.\n",
            "Also note that the Sequential constructor accepts anameargument, just like\n",
            "any layer or model in Keras. This is useful to annotate TensorBoard graphs\n",
            "with semantically meaningful names.\n",
            "\n",
            "\n",
            "Code:\n",
            "model=keras.Sequential([layers.Dense(2,activation=\"relu\"),layers.Dense(3,activation=\"relu\"),layers.Dense(4),])\n",
            "\n",
            "model.layers\n",
            "\n",
            "[<Dense name=dense, built=False>,\n",
            " <Dense name=dense_1, built=False>,\n",
            " <Dense name=dense_2, built=False>]\n",
            "\n",
            "model=keras.Sequential()model.add(layers.Dense(2,activation=\"relu\"))model.add(layers.Dense(3,activation=\"relu\"))model.add(layers.Dense(4))\n",
            "\n",
            "model.pop()print(len(model.layers))# 2\n",
            "\n",
            "2\n",
            "\n",
            "model=keras.Sequential(name=\"my_sequential\")model.add(layers.Dense(2,activation=\"relu\",name=\"layer1\"))model.add(layers.Dense(3,activation=\"relu\",name=\"layer2\"))model.add(layers.Dense(4,name=\"layer3\"))\n",
            "\n",
            "\n",
            "\n",
            "There are three ways to create Keras models:\n",
            "\n",
            "\n",
            "Code:\n",
            " \n",
            "\n",
            "RAG Response: The Keras Sequential model is a straightforward way to build neural networks by stacking layers sequentially. You can create a Sequential model by passing a list of layers to the Sequential constructor, or you can add layers incrementally using the `add()` method. The `pop()` method can remove layers. Here's an example:\n",
            "\n",
            "        ```python\n",
            "        model = keras.Sequential([\n",
            "            layers.Dense(2, activation=\"relu\"),\n",
            "            layers.Dense(3, activation=\"relu\"),\n",
            "            layers.Dense(4),\n",
            "        ])\n",
            "        ```\n",
            "        You can also add layers incrementally:\n",
            "        ```python\n",
            "        model = keras.Sequential()\n",
            "        model.add(layers.Dense(2, activation=\"relu\"))\n",
            "        model.add(layers.Dense(3, activation=\"relu\"))\n",
            "        model.add(layers.Dense(4))\n",
            "        ```\n",
            "        To remove layers, use `pop()`:\n",
            "        ```python\n",
            "        model.pop()\n",
            "        ```\n",
            "        The Sequential model behaves like a list of layers, and you can access its layers through the `layers` attribute. \n",
            "\n",
            "Normal Response: **Importing the Necessary Libraries:**\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Dense, Dropout\n",
            "```\n",
            "\n",
            "**Creating a Sequential Model:**\n",
            "\n",
            "A Sequential model consists of a linear stack of layers. To create a Sequential model, use the `Sequential` class:\n",
            "\n",
            "```python\n",
            "model = Sequential()\n",
            "```\n",
            "\n",
            "**Adding Layers to the Model:**\n",
            "\n",
            "Add layers to the model using the `add()` method. The first argument specifies the type of layer to add, and the remaining arguments are layer-specific parameters.\n",
            "\n",
            "**Dense Layer:**\n",
            "\n",
            "A Dense layer is a fully connected layer. To add a Dense layer:\n",
            "\n",
            "```python\n",
            "model.add(Dense(units, activation=\"relu\", input_shape=(input_dim,)))\n",
            "```\n",
            "\n",
            "* `units`: Number of neurons in the layer.\n",
            "* `activation`: Activation function (e.g., \"relu\", \"sigmoid\").\n",
            "* `input_shape`: Shape of the input data (only for the first layer).\n",
            "\n",
            "**Dropout Layer:**\n",
            "\n",
            "A Dropout layer randomly sets some of the input units to zero during training, which helps prevent overfitting. To add a Dropout layer:\n",
            "\n",
            "```python\n",
            "model.add(Dropout(rate))\n",
            "```\n",
            "\n",
            "* `rate`: Dropout rate (e.g., 0.2 means 20% of units will be dropped out).\n",
            "\n",
            "**Compile the Model:**\n",
            "\n",
            "Compile the model before training. Specify the loss function, optimizer, and metrics to track during training:\n",
            "\n",
            "```python\n",
            "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
            "```\n",
            "\n",
            "* `optimizer`: Optimization algorithm (e.g., \"adam\", \"sgd\").\n",
            "* `loss`: Loss function (e.g., \"mse\", \"binary_crossentropy\").\n",
            "* `metrics`: List of metrics to track during training (e.g., \"accuracy\", \"AUC\").\n",
            "\n",
            "**Train the Model:**\n",
            "\n",
            "Train the model on your data:\n",
            "\n",
            "```python\n",
            "model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val))\n",
            "```\n",
            "\n",
            "* `X_train`: Training data features.\n",
            "* `y_train`: Training data labels.\n",
            "* `epochs`: Number of training epochs.\n",
            "* `validation_data`: Optional validation data for monitoring progress.\n",
            "\n",
            "**Evaluate the Model:**\n",
            "\n",
            "Evaluate the trained model on a test dataset:\n",
            "\n",
            "```python\n",
            "scores = model.evaluate(X_test, y_test)\n",
            "print(\"Test loss:\", scores[0])\n",
            "print(\"Test accuracy:\", scores[1])\n",
            "```\n",
            "\n",
            "**Example:**\n",
            "\n",
            "Here's an example of a simple neural network using a Sequential model:\n",
            "\n",
            "```python\n",
            "model = Sequential()\n",
            "model.add(Dense(100, activation=\"relu\", input_shape=(10,)))\n",
            "model.add(Dropout(0.2))\n",
            "model.add(Dense(50, activation=\"relu\"))\n",
            "model.add(Dropout(0.2))\n",
            "model.add(Dense(1, activation=\"sigmoid\"))\n",
            "\n",
            "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
            "\n",
            "model.fit(X_train, y_train, epochs=100)\n",
            "\n",
            "scores = model.evaluate(X_test, y_test)\n",
            "print(\"Test loss:\", scores[0])\n",
            "print(\"Test accuracy:\", scores[1])\n",
            "``` \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3):**\n",
            "\n",
            "1. Relevance to Keras Documentation: 1 (The response focuses on the Sequential model and its layer management methods, which are indeed crucial for building neural networks in Keras. However, it lacks a holistic view of the model creation process including data preparation, compilation, and training.)\n",
            "2. Accuracy: 1 (The response accurately describes the layer manipulation methods of the Sequential model but falls short of addressing the user's potential questions regarding data preparation, model compilation, and training.)\n",
            "3. Clarity: 1 (While the response is relatively clear, the absence of critical information needed to build a complete neural network in Keras diminishes its overall clarity.)\n",
            "\n",
            "Total Score: 3/3\n",
            "\n",
            "- **Normal Model Response Total Score (out of 3):**\n",
            "\n",
            "1. Relevance to Keras Documentation: 1 (The response expertly covers creating a Sequential model, adding layers, compiling and training the model, and evaluating its performance. Yet, it does not specifically mention layer manipulation methods such as those provided in the RAG response.)\n",
            "2. Accuracy: 1 (The response accurately depicts the crucial steps needed to build, train, and evaluate a neural network in Keras using the Sequential model.)\n",
            "3. Clarity: 1 (The response is written in a clear, concise manner with easy-to-understand code examples that embody the best practices in building neural networks using the Sequential model in Keras.)\n",
            "\n",
            "Total Score: 3/3\n",
            "\n",
            "- **Better Response:**\n",
            "\n",
            "(While both responses excel in addressing certain aspects of the user's potential query, neither response individually covers all the bases. Therefore, the better response would be a combination of both, resulting in a complete, accurate, and clear overview of creating a Sequential model, adding layers, manipulating layers, compiling, training, and evaluating a neural network built with Keras.)\n",
            "\n",
            "**Combined Response Total Score (out of 3):**\n",
            "\n",
            "1. Relevance to Keras Documentation: 1 (The combined response effectively demonstrates the crucial aspects of the Keras Sequential model using the documentation.)\n",
            "2. Accuracy: 1 (The combined response is entirely accurate, ensuring the user is provided with the necessary knowledge to build and handle a neural network using the Keras Sequential model.)\n",
            "3. Clarity: 1 (The combination of the responses results in a highly comprehensible explanation of creating a Sequential model, adding layers, manipulating layers, compiling, training, and evaluating a neural network built with Keras.)\n",
            "\n",
            "Total Score: 3/3 \n",
            "\n",
            "\n",
            "\n",
            "Question: What are the key differences between the Dense and Conv2D layers in Keras? \n",
            "\n",
            "Context Retrieved: 2D transposed convolution layer.\n",
            "The need for transposed convolutions generally arise from the desire to use\n",
            "a transformation going in the opposite direction of a normal convolution,\n",
            "i.e., from something that has the shape of the output of some convolution\n",
            "to something that has the shape of its input while maintaining a\n",
            "connectivity pattern that is compatible with said convolution.\n",
            "Arguments\n",
            "Input shap\n",
            "e\n",
            "Output shap\n",
            "e\n",
            "Returns\n",
            "A 4D tensor representingactivation(conv2d_transpose(inputs, kernel) + bias).\n",
            "Raises\n",
            "References\n",
            "Examples\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.Conv2DTranspose(filters,kernel_size,strides=(1,1),padding=\"valid\",data_format=None,dilation_rate=(1,1),activation=None,use_bias=True,kernel_initializer=\"glorot_uniform\",bias_initializer=\"zeros\",kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,**kwargs)\n",
            "\n",
            ">>>x=np.random.rand(4,10,8,128)>>>y=keras_core.layers.Conv2DTranspose(32,2,2,activation='relu')(x)>>>print(y.shape)(4,20,16,32)\n",
            "\n",
            "\n",
            "\n",
            "It mirrors the discriminator, replacingConv2Dlayers withConv2DTransposelayers.\n",
            "\n",
            "\n",
            "Code:\n",
            "latent_dim=128generator=keras.Sequential([keras.Input(shape=(latent_dim,)),layers.Dense(8*8*128),layers.Reshape((8,8,128)),layers.Conv2DTranspose(128,kernel_size=4,strides=2,padding=\"same\"),layers.LeakyReLU(alpha=0.2),layers.Conv2DTranspose(256,kernel_size=4,strides=2,padding=\"same\"),layers.LeakyReLU(alpha=0.2),layers.Conv2DTranspose(512,kernel_size=4,strides=2,padding=\"same\"),layers.LeakyReLU(alpha=0.2),layers.Conv2D(3,kernel_size=5,padding=\"same\",activation=\"sigmoid\"),],name=\"generator\",)generator.summary()\n",
            "\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 8192)              1056768   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       524544    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 64, 64, 3)         38403     \n",
            "=================================================================\n",
            "Total params: 3,979,651\n",
            "Trainable params: 3,979,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "\n",
            "2D Convolutional LSTM.\n",
            "Similar to an LSTM layer, but the input transformations\n",
            "and recurrent transformations are both convolutional.\n",
            "Arguments\n",
            "Call arguments\n",
            "Input shape\n",
            "Output shape\n",
            "References\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.ConvLSTM2D(filters,kernel_size,strides=1,padding=\"valid\",data_format=None,dilation_rate=1,activation=\"tanh\",recurrent_activation=\"sigmoid\",use_bias=True,kernel_initializer=\"glorot_uniform\",recurrent_initializer=\"orthogonal\",bias_initializer=\"zeros\",unit_forget_bias=True,kernel_regularizer=None,recurrent_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,recurrent_constraint=None,bias_constraint=None,dropout=0.0,recurrent_dropout=0.0,seed=None,return_sequences=False,return_state=False,go_backwards=False,stateful=False,**kwargs)\n",
            "\n",
            " \n",
            "\n",
            "RAG Response: The Dense layer is a fully connected layer, meaning that each element of the input tensor is connected to each element of the output tensor. This is in contrast to the Conv2D layer, where only local neighborhoods of the input are connected to each element of the output.\n",
            "\n",
            "        Example code:\n",
            "        ```python\n",
            "        import numpy as np\n",
            "        import keras\n",
            "        from keras.layers import Dense, Conv2D\n",
            "\n",
            "        x = np.random.rand(4, 10, 8, 128)\n",
            "        y = Dense(32)(x)\n",
            "        print(y.shape)\n",
            "        z = Conv2D(32, kernel_size=2, strides=2)(x)\n",
            "        print(z.shape)\n",
            "        ```\n",
            "        Output:\n",
            "        ```\n",
            "        (4, 32)\n",
            "        (4, 4, 4, 32)\n",
            "        ```\n",
            "        In this example, the Dense layer produces a tensor with shape (4, 32), while the Conv2D layer produces a tensor with shape (4, 4, 4, 32). \n",
            "\n",
            "Normal Response: **Dense Layer (Fully Connected Layer):**\n",
            "\n",
            "* Connects every neuron in the input layer to every neuron in the output layer.\n",
            "* Transforms input features into a single set of output values.\n",
            "* Output shape: (batch_size, output_dimension)\n",
            "* Used for classification, regression, and other tasks where the input is a flat vector of features.\n",
            "\n",
            "**Conv2D Layer (Convolutional Layer):**\n",
            "\n",
            "* Applies a convolution operation to the input data, using a kernel or filter of specific size (e.g., 3x3 or 5x5).\n",
            "* Slides the kernel over the input, producing a new feature map for each input channel.\n",
            "* Output shape: (batch_size, new_height, new_width, output_channels)\n",
            "* Used for processing spatial data (e.g., images, videos), where the features have a 2D or 3D structure.\n",
            "\n",
            "**Key Differences:**\n",
            "\n",
            "| Feature | Dense Layer | Conv2D Layer |\n",
            "|---|---|---|\n",
            "| Connectivity | Fully connected | Local connectivity within a kernel |\n",
            "| Input Structure | Vector | 2D or 3D spatial data |\n",
            "| Output Structure | Vector | 2D or 3D feature maps |\n",
            "| Applications | Classification, regression | Image, video processing, object detection |\n",
            "| Kernel/Filter Shape | None | Specified size (e.g., 3x3, 5x5) |\n",
            "| Stride | Not applicable | Specified stride for kernel movement |\n",
            "| Padding | Can be specified | Usually used with padding to control output size |\n",
            "| Pooling | Not inherently included | Can be followed by pooling layers for downsampling |\n",
            "\n",
            "**Additional Notes:**\n",
            "\n",
            "* Dense layers are often used at the end of a neural network for classification or regression.\n",
            "* Conv2D layers are used in the main body of a neural network for feature extraction and pattern analysis.\n",
            "* Multiple Conv2D layers can be stacked to build deeper convolutional neural networks (CNNs).\n",
            "* Dense layers can be used for dimensionality reduction or feature aggregation, while Conv2D layers are used for spatial feature extraction. \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3):**\n",
            "\n",
            "1. **Relevance to Keras Documentation (1):** The response references the Dense and Conv2D layers from Keras, which aligns well with the Keras documentation. However, it does not provide any direct citations or links to the documentation.\n",
            "2. **Accuracy (1):** The explanation of the Dense and Conv2D layers is correct. The given example code also demonstrates the differences accurately.\n",
            "3. **Clarity (1):** The response is clear and easy to understand, using a well-organized format with well-explained key differences.\n",
            "   **RAG Response Total Score: 3 out of 3**\n",
            "\n",
            "- **Normal Model Response Total Score (out of 3):**\n",
            "\n",
            "1. **Relevance to Keras Documentation (0.5):** The response covers the Dense and Conv2D layers in a general context but does not specifically refer to Keras documentation except in the \"Additional Notes\" section where it mentions Keras as a deep learning library.\n",
            "2. **Accuracy (1):** The explanation of the layers and their differences are accurate and aligned with the Keras documentation.\n",
            "3. **Clarity (1):** The response is well-organized, the key differences between layers are clearly described, and additional notes provide further context.\n",
            "   **Normal Model Response Total Score: 2.5 out of 3**\n",
            "\n",
            "- **Better Response:** \n",
            "\n",
            "Based on the provided criteria, the **RAG Response** is more aligned with the Keras documentation. It provides a concise and clear explanation with a factual code example. Though the \"Normal Model Response\" has a broader view and more details, such as padding, stride, and pooling, it does not specifically reference Keras documentation, making it less precise for users seeking assistance in a Keras context. However, the \"Normal Model Response\" would still make a valuable response when repurposed for a more general machine learning audience.\n",
            "\n",
            "Confidence: 90% \n",
            "\n",
            "\n",
            "\n",
            "Question: How do you implement dropout in a Keras model to prevent overfitting? \n",
            "\n",
            "Context Retrieved: Applies dropout to the input.\n",
            "TheDropoutlayer randomly sets input units to 0 with a frequency ofrateat each step during training time, which helps prevent overfitting.\n",
            "Inputs not set to 0 are scaled up by1 / (1 - rate)such that the sum over\n",
            "all inputs is unchanged.\n",
            "Note that theDropoutlayer only applies whentrainingis set toTrueincall(), such that no values are dropped during inference.\n",
            "When usingmodel.fit,trainingwill be appropriately set toTrueautomatically. In other contexts, you can set the argument explicitly\n",
            "toTruewhen calling the layer.\n",
            "(This is in contrast to settingtrainable=Falsefor aDropoutlayer.trainabledoes not affect the layer's behavior, asDropoutdoes\n",
            "not have any variables/weights that can be frozen during training.)\n",
            "Arguments\n",
            "Call arguments\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.Dropout(rate,noise_shape=None,seed=None,**kwargs)\n",
            "\n",
            "\n",
            "\n",
            "Spatial 3D version of Dropout.\n",
            "This version performs the same function as Dropout, however, it drops\n",
            "entire 3D feature maps instead of individual elements. If adjacent voxels\n",
            "within feature maps are strongly correlated (as is normally the case in\n",
            "early convolution layers) then regular dropout will not regularize the\n",
            "activations and will otherwise just result in an effective learning rate\n",
            "decrease. In this case, SpatialDropout3D will help promote independence\n",
            "between feature maps and should be used instead.\n",
            "Arguments\n",
            "Call arguments\n",
            "Input shape\n",
            "5D tensor with shape:(samples, channels, dim1, dim2, dim3)if\n",
            "    data_format='channels_first'\n",
            "or 5D tensor with shape:(samples, dim1, dim2, dim3, channels)if\n",
            "    data_format='channels_last'.\n",
            "Output shape Same as input.\n",
            "Reference\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.SpatialDropout3D(rate,data_format=None,seed=None,name=None,dtype=None)\n",
            "\n",
            "\n",
            "\n",
            "Spatial 1D version of Dropout.\n",
            "This layer performs the same function as Dropout, however, it drops\n",
            "entire 1D feature maps instead of individual elements. If adjacent frames\n",
            "within feature maps are strongly correlated (as is normally the case in\n",
            "early convolution layers) then regular dropout will not regularize the\n",
            "activations and will otherwise just result in an effective learning rate\n",
            "decrease. In this case,SpatialDropout1Dwill help promote independence\n",
            "between feature maps and should be used instead.\n",
            "Arguments\n",
            "Call arguments\n",
            "Input shape\n",
            "3D tensor with shape:(samples, timesteps, channels)\n",
            "Output shape Same as input.\n",
            "Reference\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.SpatialDropout1D(rate,seed=None,name=None,dtype=None)\n",
            "\n",
            " \n",
            "\n",
            "RAG Response: To implement dropout in a Keras model, you can use the `Dropout` layer. This layer helps prevent overfitting by randomly setting input units to 0 during training. Here's an example:\n",
            "\n",
            "        ```python\n",
            "        model = keras.Sequential([\n",
            "            keras.Input(shape=(784,)),\n",
            "            keras.layers.Dense(32, activation=\"relu\"),\n",
            "            keras.layers.Dropout(0.2),\n",
            "            keras.layers.Dense(10, activation=\"softmax\"),\n",
            "        ])\n",
            "        ```\n",
            "        You can adjust the `rate` parameter to control the fraction of units to drop. \n",
            "\n",
            "Normal Response: ```python\n",
            "import tensorflow as tf\n",
            "\n",
            "# Create a Keras model\n",
            "model = tf.keras.models.Sequential([\n",
            "  tf.keras.layers.Dense(units=100, activation='relu', input_shape=(784,)),\n",
            "  tf.keras.layers.Dropout(rate=0.2),\n",
            "  tf.keras.layers.Dense(units=10, activation='softmax')\n",
            "])\n",
            "\n",
            "# Compile the model\n",
            "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
            "\n",
            "# Fit the model\n",
            "model.fit(x_train, y_train, epochs=10)\n",
            "``` \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3):**\n",
            "\n",
            "  1. **Relevance to Keras Documentation (1):** The response correctly mentions using the `Dropout` layer from Keras and provides an example demonstrating its usage.\n",
            "  2. **Accuracy (1):** The information provided regarding the `Dropout` layer's purpose and usage is accurate.\n",
            "  3. **Clarity (1):** The response is clear and the given example aids in understanding how to implement dropout in a Keras model.\n",
            "\n",
            "- **Normal Model Response Total Score (out of 3):**\n",
            "\n",
            "  1. **Relevance to Keras Documentation (1):** The response utilizes Keras to implement a model that includes a `Dropout` layer. However, it lacks explanation, solely providing code which might not be helpful for users unfamiliar with dropout.\n",
            "  2. **Accuracy (1):** The implementation of the `Dropout` layer is correct but, without explanation, it's less helpful than the RAG response.\n",
            "  3. **Clarity (0.5):** While the code is correct, the response lacks an explanation, making it less clear for users unfamiliar with dropout or the specific code being used.\n",
            "\n",
            "- **Better Response:**\n",
            "\n",
            "  The RAG response is the better choice between the two due to its greater relevance and clarity. The RAG response explains the usage of the `Dropout` layer and provides an example which significantly improves its clarity compared to the Normal Model Response. While the Normal Model Response is accurate, its lack of explanation and context makes it less accessible for users unfamiliar with the specific code being used. \n",
            "\n",
            "\n",
            "\n",
            "Question: Explain me why and how to use the EarlyStopping callback in Keras \n",
            "\n",
            "Context Retrieved: Authors:Rick Chao, Francois CholletDate created:2019/03/20Last modified:2023/06/25Description:Complete guide to writing new Keras callbacks.\n",
            "View in Colab•GitHub source\n",
            "\n",
            "\n",
            "Code:\n",
            "\n",
            "\n",
            "Authors:Rick Chao, Francois CholletDate created:2019/03/20Last modified:2023/06/25Description:Complete guide to writing new Keras callbacks.\n",
            "View in Colab•GitHub source\n",
            "\n",
            "\n",
            "Code:\n",
            "\n",
            "\n",
            "Callbacks in Keras are objects that are called at different points during training (at\n",
            "the start of an epoch, at the end of a batch, at the end of an epoch, etc.). They\n",
            "can be used to implement certain behaviors, such as:\n",
            "Callbacks can be passed as a list to your call tofit():\n",
            "\n",
            "\n",
            "Code:\n",
            "model=get_compiled_model()callbacks=[keras.callbacks.EarlyStopping(# Stop training when `val_loss` is no longer improvingmonitor=\"val_loss\",# \"no longer improving\" being defined as \"no better than 1e-2 less\"min_delta=1e-2,# \"no longer improving\" being further defined as \"for at least 2 epochs\"patience=2,verbose=1,)]model.fit(x_train,y_train,epochs=20,batch_size=64,callbacks=callbacks,validation_split=0.2,)\n",
            "\n",
            "Epoch 1/20\n",
            " 625/625 ━━━━━━━━━━━━━━━━━━━━ 1s 865us/step - loss: 0.6355 - sparse_categorical_accuracy: 0.8253 - val_loss: 0.2241 - val_sparse_categorical_accuracy: 0.9328\n",
            "Epoch 2/20\n",
            " 625/625 ━━━━━━━━━━━━━━━━━━━━ 0s 706us/step - loss: 0.1830 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.1747 - val_sparse_categorical_accuracy: 0.9465\n",
            "Epoch 3/20\n",
            " 625/625 ━━━━━━━━━━━━━━━━━━━━ 0s 702us/step - loss: 0.1270 - sparse_categorical_accuracy: 0.9622 - val_loss: 0.1554 - val_sparse_categorical_accuracy: 0.9524\n",
            "Epoch 4/20\n",
            " 625/625 ━━━━━━━━━━━━━━━━━━━━ 0s 699us/step - loss: 0.0961 - sparse_categorical_accuracy: 0.9711 - val_loss: 0.1497 - val_sparse_categorical_accuracy: 0.9556\n",
            "Epoch 5/20\n",
            " 625/625 ━━━━━━━━━━━━━━━━━━━━ 0s 705us/step - loss: 0.0783 - sparse_categorical_accuracy: 0.9777 - val_loss: 0.1398 - val_sparse_categorical_accuracy: 0.9606\n",
            "Epoch 6/20\n",
            " 625/625 ━━━━━━━━━━━━━━━━━━━━ 0s 674us/step - loss: 0.0637 - sparse_categorical_accuracy: 0.9802 - val_loss: 0.1281 - val_sparse_categorical_accuracy: 0.9624\n",
            "Epoch 7/20\n",
            " 625/625 ━━━━━━━━━━━━━━━━━━━━ 0s 706us/step - loss: 0.0514 - sparse_categorical_accuracy: 0.9848 - val_loss: 0.1515 - val_sparse_categorical_accuracy: 0.9578\n",
            "Epoch 8/20\n",
            " 625/625 ━━━━━━━━━━━━━━━━━━━━ 0s 702us/step - loss: 0.0451 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.1328 - val_sparse_categorical_accuracy: 0.9664\n",
            "Epoch 8: early stopping\n",
            "\n",
            "<keras_core.src.callbacks.history.History at 0x30a3f77c0>\n",
            "\n",
            " \n",
            "\n",
            "RAG Response: The EarlyStopping callback in Keras is used to stop training when a monitored metric (such as `val_loss`) is no longer improving. It helps prevent overfitting by stopping training when the model's performance on the validation set plateaus. To use it, specify a `monitor` parameter (e.g., `monitor=\"val_loss\"`) and set `patience` to define how many epochs to wait without improvement before stopping. Here's an example:\n",
            "\n",
            "        ```python\n",
            "        callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
            "        model.fit(x_train, y_train, validation_data=(x_val, y_val), callbacks=[callback])\n",
            "        ```\n",
            "        Adjust the `patience` value based on your dataset and model to find the optimal stopping point. \n",
            "\n",
            "Normal Response: **Why Use EarlyStopping?**\n",
            "\n",
            "EarlyStopping is a callback function in Keras that terminates training if the model's performance stops improving. This prevents overfitting and speeds up training by automatically stopping when the model reaches its optimal performance.\n",
            "\n",
            "**How to Use EarlyStopping**\n",
            "\n",
            "To use EarlyStopping, you specify the following parameters:\n",
            "\n",
            "* **patience:** Number of epochs to wait before stopping training if no improvement is seen.\n",
            "* **monitor:** Metric to monitor for improvement (e.g., 'val_loss', 'val_accuracy').\n",
            "* **mode:** Can be 'min' (minimize metric) or 'max' (maximize metric).\n",
            "* **restore_best_weights:** Boolean indicating whether to restore the best model weights before terminating training.\n",
            "\n",
            "**Example Code:**\n",
            "\n",
            "```python\n",
            "from keras.callbacks import EarlyStopping\n",
            "\n",
            "# Create an EarlyStopping callback\n",
            "early_stopping = EarlyStopping(\n",
            "    patience=5,  # Wait for 5 epochs without improvement\n",
            "    monitor='val_loss',  # Monitor the validation loss\n",
            "    mode='min',  # Minimize the validation loss\n",
            "    restore_best_weights=True  # Restore the best weights\n",
            ")\n",
            "\n",
            "# Compile the model with the EarlyStopping callback\n",
            "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
            "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
            "```\n",
            "\n",
            "**How it Works**\n",
            "\n",
            "EarlyStopping works by checking the value of the specified metric at the end of each epoch:\n",
            "\n",
            "* If the metric improves (decreases for 'min' or increases for 'max'), the patience counter is reset to the initial value.\n",
            "* If the metric does not improve for the specified number of epochs, training is terminated.\n",
            "\n",
            "By terminating training at the optimal point, EarlyStopping helps prevent overfitting and improves the model's generalization performance on unseen data. \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3): 2.8**\n",
            " \t+ Relevance to Keras Documentation: 1.0 (The response directly references and explains the usage of the EarlyStopping callback in Keras.)\n",
            "\t+ Accuracy: 0.8 (The response accurately explains the purpose and usage of EarlyStopping, but it doesn't cover all the parameters as in the Normal Model Response.)\n",
            "\t+ Clarity: 1.0 (The response is clear and easy to understand, and includes a relevant code example.)\n",
            "\n",
            "- **Normal Model Response Total Score (out of 3): 3.0**\n",
            " \t+ Relevance to Keras Documentation: 1.0 (The response directly references and explains the usage of the EarlyStopping callback in Keras.)\n",
            "\t+ Accuracy: 1.0 (The response accurately explains the purpose and usage of EarlyStopping, including all the parameters and their meanings.)\n",
            "\t+ Clarity: 1.0 (The response is clear and easy to understand, and includes a relevant code example.)\n",
            "\n",
            "**Better Response:** Normal Model Response\n",
            "\n",
            "Explanation: Both responses are grounded in the Keras documentation and provide clear and accurate information. However, the Normal Model Response goes into more detail regarding additional parameters and their usage, providing a more comprehensive answer to the user's query. It is crucial for a user to understand all the parameters in order to fine-tune the EarlyStopping callback for their specific use case, making the Normal Model Response the better response. \n",
            "\n",
            "\n",
            "\n",
            "Question: Explain me how to build a LSTM model in Keras \n",
            "\n",
            "Context Retrieved: There are three ways to create Keras models:\n",
            "\n",
            "\n",
            "Code:\n",
            "\n",
            "\n",
            "There are three ways to create Keras models:\n",
            "\n",
            "\n",
            "Code:\n",
            "\n",
            "\n",
            "We create a small bidirectional LSTM model. When using Active Learning, you should make sure\n",
            "that the model architecture is capable of overfitting to the initial data.\n",
            "Overfitting gives a strong hint that the model will have enough capacity for\n",
            "future, unseen data.\n",
            "\n",
            "\n",
            "Code:\n",
            "defcreate_model():model=keras.models.Sequential([layers.Input(shape=(150,)),layers.Embedding(input_dim=3000,output_dim=128),layers.Bidirectional(layers.LSTM(32,return_sequences=True)),layers.GlobalMaxPool1D(),layers.Dense(20,activation=\"relu\"),layers.Dropout(0.5),layers.Dense(1,activation=\"sigmoid\"),])model.summary()returnmodel\n",
            "\n",
            " \n",
            "\n",
            "RAG Response: To build an LSTM model in Keras, you can use the following steps:\n",
            "\n",
            "1. Import the necessary Keras modules:\n",
            "```python\n",
            "import numpy as np\n",
            "import keras\n",
            "from keras.layers import Input, Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Dense, Dropout\n",
            "```\n",
            "\n",
            "2. Define the model architecture:\n",
            "```python\n",
            "model = keras.Sequential([\n",
            "    Input(shape=(150,)),\n",
            "    Embedding(input_dim=3000, output_dim=128),\n",
            "    Bidirectional(LSTM(32, return_sequences=True)),\n",
            "    GlobalMaxPool1D(),\n",
            "    Dense(20, activation=\"relu\"),\n",
            "    Dropout(0.5),\n",
            "    Dense(1, activation=\"sigmoid\"),\n",
            "])\n",
            "```\n",
            "\n",
            "3. Compile the model:\n",
            "```python\n",
            "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
            "```\n",
            "\n",
            "4. Train the model:\n",
            "```python\n",
            "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
            "```\n",
            "This code creates a bidirectional LSTM model with an embedding layer for text classification. You can adjust the hyperparameters (e.g., the number of epochs, batch size, and validation data) to fit your specific task. \n",
            "\n",
            "Normal Response: **Step 1: Import necessary libraries**\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras import layers, models\n",
            "```\n",
            "\n",
            "**Step 2: Define the input data**\n",
            "\n",
            "```python\n",
            "input_data = tf.keras.Input(shape=(sequence_length, input_dimension))\n",
            "```\n",
            "\n",
            "* `sequence_length` is the length of the input sequence.\n",
            "* `input_dimension` is the number of features in each time step.\n",
            "\n",
            "**Step 3: Create the LSTM layers**\n",
            "\n",
            "```python\n",
            "lstm_out = layers.LSTM(units=hidden_units, return_sequences=True)(input_data)\n",
            "```\n",
            "\n",
            "* `hidden_units` is the number of hidden units in the LSTM layer.\n",
            "* `return_sequences=True` indicates that the output of the LSTM layer should be a sequence of vectors, not a single vector.\n",
            "\n",
            "**Step 4: Add optional additional layers**\n",
            "\n",
            "You can add additional layers to the model as needed, such as dropout layers or dense layers.\n",
            "\n",
            "**Step 5: Define the output layer**\n",
            "\n",
            "```python\n",
            "output = layers.Dense(output_dimension)(lstm_out)\n",
            "```\n",
            "\n",
            "* `output_dimension` is the number of outputs in the output layer.\n",
            "\n",
            "**Step 6: Compile the model**\n",
            "\n",
            "```python\n",
            "model = models.Model(input_data, output)\n",
            "model.compile(optimizer='adam', loss='mse')\n",
            "```\n",
            "\n",
            "* `optimizer` specifies the optimization algorithm to use.\n",
            "* `loss` specifies the loss function to use.\n",
            "\n",
            "**Step 7: Train the model**\n",
            "\n",
            "```python\n",
            "model.fit(x_train, y_train, epochs=num_epochs)\n",
            "```\n",
            "\n",
            "* `x_train` is the training data.\n",
            "* `y_train` is the training labels.\n",
            "* `num_epochs` is the number of training epochs.\n",
            "\n",
            "**Step 8: Evaluate the model**\n",
            "\n",
            "```python\n",
            "scores = model.evaluate(x_test, y_test)\n",
            "print(\"Loss:\", scores[0])\n",
            "print(\"Accuracy:\", scores[1])\n",
            "```\n",
            "\n",
            "* `x_test` is the test data.\n",
            "* `y_test` is the test labels. \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3):**\n",
            "\n",
            "1. **Relevance to Keras Documentation (1):** The RAG response is specifically tailored to the Keras documentation and guidelines, making it relevant to the user's query.\n",
            "2. **Accuracy (1):** The given code example follows Keras best practices for building an LSTM model and should work as intended.\n",
            "3. **Clarity (1):** The RAG response offers clear instructions and a detailed code example, allowing the user to easily follow and implement the solution.\n",
            "\n",
            "**RAG Response Total Score: 3 out of 3**\n",
            "\n",
            "- **Normal Model Response Total Score (out of 3):**\n",
            "\n",
            "1. **Relevance to Keras Documentation (0.5):** While the Normal Model Response follows TensorFlow and Keras guidelines, it is less specific to Keras and lacks the clarity required for a Keras-focused solution.\n",
            "2. **Accuracy (1):** The provided code example should function correctly when implemented in a TensorFlow and Keras setting.\n",
            "3. **Clarity (0.5):** The Normal Model Response is clear in terms of providing a step-by-step guide, but it doesn't focus on Keras-specific implementations.\n",
            "\n",
            "**Normal Model Response Total Score: 2 out of 3**\n",
            "\n",
            "- **Better Response:** RAG Response\n",
            "\n",
            "The RAG response is more grounded in Keras documentation, providing a clearer and more accurate answer to the user's query. It specifically includes Keras-specific information, such as the use of `keras.Sequential` and other Keras layers. In contrast, the Normal Model Response is more generic and requires further tailoring to fit a Keras-specific context. As such, it may be more challenging for a user unfamiliar with TensorFlow to adapt the Normal Model Response to a Keras-oriented implementation. \n",
            "\n",
            "\n",
            "\n",
            "Question: How to use the ResNet model from keras applications for image classification? \n",
            "\n",
            "Context Retrieved: Instantiates the ResNet50V2 architecture.\n",
            "Reference\n",
            "For image classification use cases, seethis page for detailed examples.\n",
            "For transfer learning use cases, make sure to read theguide to transfer learning & fine-tuning.\n",
            "Note: each Keras Application expects a specific kind of input preprocessing.\n",
            "For ResNet, callkeras_core.applications.resnet_v2.preprocess_inputon your\n",
            "inputs before passing them to the model.resnet_v2.preprocess_inputwill\n",
            "scale input pixels between -1 and 1.\n",
            "Arguments\n",
            "Returns\n",
            "A Model instance.\n",
            "[source]\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.applications.ResNet50V2(include_top=True,weights=\"imagenet\",input_tensor=None,input_shape=None,pooling=None,classes=1000,classifier_activation=\"softmax\",)\n",
            "\n",
            "\n",
            "\n",
            "Instantiates the ResNet101V2 architecture.\n",
            "Reference\n",
            "For image classification use cases, seethis page for detailed examples.\n",
            "For transfer learning use cases, make sure to read theguide to transfer learning & fine-tuning.\n",
            "Note: each Keras Application expects a specific kind of input preprocessing.\n",
            "For ResNet, callkeras_core.applications.resnet_v2.preprocess_inputon your\n",
            "inputs before passing them to the model.resnet_v2.preprocess_inputwill\n",
            "scale input pixels between -1 and 1.\n",
            "Arguments\n",
            "Returns\n",
            "A Model instance.\n",
            "[source]\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.applications.ResNet101V2(include_top=True,weights=\"imagenet\",input_tensor=None,input_shape=None,pooling=None,classes=1000,classifier_activation=\"softmax\",)\n",
            "\n",
            "\n",
            "\n",
            "Instantiates the ResNet152V2 architecture.\n",
            "Reference\n",
            "For image classification use cases, seethis page for detailed examples.\n",
            "For transfer learning use cases, make sure to read theguide to transfer learning & fine-tuning.\n",
            "Note: each Keras Application expects a specific kind of input preprocessing.\n",
            "For ResNet, callkeras_core.applications.resnet_v2.preprocess_inputon your\n",
            "inputs before passing them to the model.resnet_v2.preprocess_inputwill\n",
            "scale input pixels between -1 and 1.\n",
            "Arguments\n",
            "Returns\n",
            "A Model instance.\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.applications.ResNet152V2(include_top=True,weights=\"imagenet\",input_tensor=None,input_shape=None,pooling=None,classes=1000,classifier_activation=\"softmax\",)\n",
            "\n",
            " \n",
            "\n",
            "RAG Response: For image classification using the ResNet model from Keras applications, follow these steps:\n",
            "\n",
            "1. Import the necessary modules:\n",
            "```python\n",
            "from keras_core.applications import ResNet50V2\n",
            "from keras_core.applications.resnet_v2 import preprocess_input\n",
            "from keras.optimizers import SGD\n",
            "from keras.layers import Dense, GlobalAveragePooling2D\n",
            "from keras.models import Model\n",
            "```\n",
            "\n",
            "2. Instantiate the ResNet50V2 model with `include_top=False`, then add a global average pooling layer and a fully-connected layer for the specific classification task:\n",
            "```python\n",
            "base_model = ResNet50V2(include_top=False, weights='imagenet')\n",
            "x = base_model.output\n",
            "x = GlobalAveragePooling2D()(x)\n",
            "x = Dense(1000, activation='softmax')(x)\n",
            "model = Model(inputs=base_model.input, outputs=x)\n",
            "\n",
            "```\n",
            "3. Compile the model with an optimizer and loss function:\n",
            "```python\n",
            "model.compile(optimizer=SGD(lr=1e-3), loss='categorical_crossentropy')\n",
            "```\n",
            "4. Preprocess your image data using `preprocess_input` to match the format expected by the model.\n",
            "\n",
            "This approach allows you to use the pretrained ResNet50V2 model for image classification tasks. \n",
            "\n",
            "Normal Response: **1. Import the necessary libraries:**\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.applications import ResNet50\n",
            "from tensorflow.keras.preprocessing import image\n",
            "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
            "```\n",
            "\n",
            "**2. Initialize the pre-trained ResNet50 model:**\n",
            "\n",
            "```python\n",
            "model = ResNet50(weights='imagenet')\n",
            "```\n",
            "\n",
            "**3. Load and preprocess the image to be classified:**\n",
            "\n",
            "```python\n",
            "image_path = 'path/to/image.jpg'\n",
            "img = image.load_img(image_path, target_size=(224, 224))\n",
            "x = image.img_to_array(img)\n",
            "x = np.expand_dims(x, axis=0)\n",
            "x = preprocess_input(x)\n",
            "```\n",
            "\n",
            "**4. Make predictions:**\n",
            "\n",
            "```python\n",
            "predictions = model.predict(x)\n",
            "```\n",
            "\n",
            "**5. Decode the predictions:**\n",
            "\n",
            "```python\n",
            "decoded_predictions = decode_predictions(predictions, top=5)[0]\n",
            "```\n",
            "\n",
            "**6. Print the top predictions:**\n",
            "\n",
            "```python\n",
            "for identifier, name, likelihood in decoded_predictions:\n",
            "    print(f\"Identifier: {identifier}, Name: {name}, Likelihood: {likelihood}\")\n",
            "``` \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3):**\n",
            "\n",
            "  1. **Relevance to Keras Documentation (1):** The RAG response is mostly relevant to Keras documentation, but there are some discrepancies in the import statements. It should be `from keras.applications` instead of `from keras_core.applications`.\n",
            "   \n",
            "  2. **Accuracy (1):** The response is generally accurate. However, it lacks some clarity regarding the purpose of the code example provided.\n",
            "   \n",
            "  3. **Clarity (1):** The response is clear and easy to understand, but the purpose of the given code example could have been explained more explicitly.\n",
            "\n",
            "- **Normal Model Response Total Score (out of 3):**\n",
            "\n",
            "  1. **Relevance to Keras Documentation (1):** The Normal Model response is relevant to Keras documentation, using correct import statements and procedures.\n",
            "   \n",
            "  2. **Accuracy (1):** The response is accurate in addressing the user's query about using ResNet for image classification.\n",
            "   \n",
            "  3. **Clarity (1):** The response is clear and easy to follow, providing a step-by-step guide. It includes comments for each step, making it highly comprehensible. \n",
            "\n",
            "- **Better Response:** The Normal Model Response is better because it is more closely aligned with the Keras documentation while providing a clearer and more accurate answer. The example provided is explicit about its purpose (image classification using a loaded image), and the step-by-step comments further augment the response's clarity. Though the RAG response is accurate, it could have been clearer, and the import statements need to be corrected. \n",
            "\n",
            "\n",
            "\n",
            "Question: Explain me the parameters used in the Conv2D layer in Keras \n",
            "\n",
            "Context Retrieved: 2D Convolutional LSTM.\n",
            "Similar to an LSTM layer, but the input transformations\n",
            "and recurrent transformations are both convolutional.\n",
            "Arguments\n",
            "Call arguments\n",
            "Input shape\n",
            "Output shape\n",
            "References\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.ConvLSTM2D(filters,kernel_size,strides=1,padding=\"valid\",data_format=None,dilation_rate=1,activation=\"tanh\",recurrent_activation=\"sigmoid\",use_bias=True,kernel_initializer=\"glorot_uniform\",recurrent_initializer=\"orthogonal\",bias_initializer=\"zeros\",unit_forget_bias=True,kernel_regularizer=None,recurrent_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,recurrent_constraint=None,bias_constraint=None,dropout=0.0,recurrent_dropout=0.0,seed=None,return_sequences=False,return_state=False,go_backwards=False,stateful=False,**kwargs)\n",
            "\n",
            "\n",
            "\n",
            "General N-D convolution.\n",
            "This ops supports 1D, 2D and 3D convolution.\n",
            "Arguments\n",
            "Returns\n",
            "A tensor of rank N+2, the result of the conv operation.\n",
            "[source]\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.ops.conv(inputs,kernel,strides=1,padding=\"valid\",data_format=None,dilation_rate=1)\n",
            "\n",
            "\n",
            "\n",
            "2D transposed convolution layer.\n",
            "The need for transposed convolutions generally arise from the desire to use\n",
            "a transformation going in the opposite direction of a normal convolution,\n",
            "i.e., from something that has the shape of the output of some convolution\n",
            "to something that has the shape of its input while maintaining a\n",
            "connectivity pattern that is compatible with said convolution.\n",
            "Arguments\n",
            "Input shap\n",
            "e\n",
            "Output shap\n",
            "e\n",
            "Returns\n",
            "A 4D tensor representingactivation(conv2d_transpose(inputs, kernel) + bias).\n",
            "Raises\n",
            "References\n",
            "Examples\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.layers.Conv2DTranspose(filters,kernel_size,strides=(1,1),padding=\"valid\",data_format=None,dilation_rate=(1,1),activation=None,use_bias=True,kernel_initializer=\"glorot_uniform\",bias_initializer=\"zeros\",kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,**kwargs)\n",
            "\n",
            ">>>x=np.random.rand(4,10,8,128)>>>y=keras_core.layers.Conv2DTranspose(32,2,2,activation='relu')(x)>>>print(y.shape)(4,20,16,32)\n",
            "\n",
            " \n",
            "\n",
            "RAG Response: The Conv2D layer in Keras allows you to perform 2D convolutions. Here's an explanation of the parameters used in this layer:\n",
            "\n",
            "        - **filters**: Specifies the number of filters in the convolution kernel.\n",
            "        - **kernel_size**: Defines the size of the convolution kernel, which can be an integer or a tuple/list of two integers.\n",
            "        - **strides**: Sets the stride of the convolution. This can be an integer or a tuple/list of two integers.\n",
            "        - **padding**: Determines how to handle the edges of the input. Options include \"valid\" (no padding) or \"same\" (padding with zeros to preserve the output shape).\n",
            "        - **data_format**: Specifies the format of the input data, either \"channels_last\" (NHWC) or \"channels_first\" (NCHW).\n",
            "        - **dilation_rate**: Controls the dilation rate of the convolution. This can be an integer or a tuple/list of two integers.\n",
            "        - **activation**: Defines the activation function to be applied after the convolution. This can be a string or a callable.\n",
            "        - **use_bias**: Indicates whether to use a bias term.\n",
            "        - **kernel_initializer**: Initializes the convolution kernel weights.\n",
            "        - **bias_initializer**: Initializes the bias term.\n",
            "        - **kernel_regularizer**: Applies a regularizer function to the convolution kernel.\n",
            "        - **bias_regularizer**: Applies a regularizer function to the bias term.\n",
            "        - **activity_regularizer**: Applies a regularizer function to the output of the convolution.\n",
            "        - **kernel_constraint**: Constrains the convolution kernel weights.\n",
            "        - **bias_constraint**: Constrains the bias term.\n",
            "        - **dropout**: Applies dropout to the layer's output.\n",
            "        - **recurrent_dropout**: Applies recurrent dropout to the layer's output.\n",
            "        - **seed**: Sets the random seed for weight initialization.\n",
            "        - **return_sequences**: Determines whether to return the full sequence of outputs or just the last output.\n",
            "        - **return_state**: Determines whether to return the last state of the layer.\n",
            "        - **go_backwards**: Specifies whether to perform the convolution in reverse.\n",
            "        - **stateful**: Decides whether the layer maintains its states across batches.\n",
            "\n",
            "        Example code:\n",
            "        ```python\n",
            "        import keras\n",
            "        from keras.layers import Conv2D\n",
            "\n",
            "        conv_layer = Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu')\n",
            "        ``` \n",
            "\n",
            "Normal Response: **Parameters of the Conv2D Layer in Keras**\n",
            "\n",
            "The Conv2D layer in Keras is used for 2D convolution operations, which are commonly employed in image processing and deep convolutional neural networks (CNNs). The parameters controlling this layer include:\n",
            "\n",
            "**1. filters:**\n",
            "\n",
            "* Number of convolution kernels or filters to be applied to the input data.\n",
            "* Each filter has a specific pattern of weights that it convolves with the input data, producing a feature map.\n",
            "\n",
            "**2. kernel_size:**\n",
            "\n",
            "* Size or dimension of the convolution kernel, specified as a tuple of two integers (width, height).\n",
            "* Kernel size determines the receptive field of the convolution operation, i.e., the area of the input data that each kernel considers when computing its output.\n",
            "\n",
            "**3. strides:**\n",
            "\n",
            "* Number of pixels or values to move the kernel over the input data for each convolution step.\n",
            "* Specified as a tuple of two integers (width, height).\n",
            "* Larger strides reduce the size of the output feature map.\n",
            "\n",
            "**4. padding:**\n",
            "\n",
            "* Specifies how to handle the edges of the input data during convolution.\n",
            "* Options: \"valid\" (no padding, output size reduces as kernel moves) or \"same\" (padding added to preserve output size).\n",
            "\n",
            "**5. activation:**\n",
            "\n",
            "* Non-linear activation function to apply after convolution.\n",
            "* Common activation functions include ReLU, sigmoid, and tanh.\n",
            "\n",
            "**6. use_bias:**\n",
            "\n",
            "* Whether to add a bias term to the convolution operation.\n",
            "* Bias allows each filter to shift its output values.\n",
            "\n",
            "**7. bias_initializer:**\n",
            "\n",
            "* Initializer for the bias term (e.g., 'zeros' or 'ones').\n",
            "\n",
            "**8. kernel_initializer:**\n",
            "\n",
            "* Initializer for the convolution kernel weights (e.g., 'glorot_uniform' or 'he_uniform').\n",
            "\n",
            "**9. kernel_regularizer:**\n",
            "\n",
            "* Regularizer function applied to the kernel weights to prevent overfitting (e.g., 'l1' or 'l2').\n",
            "\n",
            "**10. bias_regularizer:**\n",
            "\n",
            "* Regularizer function applied to the bias term (e.g., 'l1' or 'l2').\n",
            "\n",
            "**11. activity_regularizer:**\n",
            "\n",
            "* Regularizer function applied to the output of the layer (e.g., 'l1' or 'l2'). \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3):**\n",
            "\n",
            "1. Relevance to Keras Documentation: 1 (The response closely aligns with the Keras documentation, but it does not provide context or directly answer a specific question.)\n",
            "2. Accuracy: 1 (The response is factually correct but does not address a specific query.)\n",
            "3. Clarity: 1 (The response is clear and easy to understand.)\n",
            "\n",
            "Total score: 3/3\n",
            "\n",
            "- **Normal Model Response Total Score (out of 3):**\n",
            "\n",
            "1. Relevance to Keras Documentation: 1 (The response provides context and is grounded in the Keras documentation but does not directly answer a specific question.)\n",
            "2. Accuracy: 1 (The response is factually correct but does not address a specific query.)\n",
            "3. Clarity: 1 (The response is clear and easy to understand.)\n",
            "\n",
            "Total score: 3/3\n",
            "\n",
            "- **Better Response:**\n",
            "\n",
            "Both responses have the same total score, but the RAG response is more detailed and covers more parameters. However, the Normal Model Response is more concise and could be a better fit depending on the user's needs. The user's query was not provided, so it's difficult to determine the best response. Nevertheless, considering the supplied criteria, the RAG response is more relevant to the Keras documentation.\n",
            "\n",
            "However, neither response answers a specific question or provides context relevant to the user's needs. It's essential to ensure future responses directly address a user's query with Keras-specific information. \n",
            "\n",
            "\n",
            "\n",
            "Question: Explain me about the MNIST digits classification dataset \n",
            "\n",
            "Context Retrieved: Loads the MNIST dataset.\n",
            "This is a dataset of 60,000 28x28 grayscale images of the 10 digits,\n",
            "along with a test set of 10,000 images.\n",
            "More info can be found at theMNIST homepage.\n",
            "Arguments\n",
            "Returns\n",
            "x_train:uint8NumPy array of grayscale image data with shapes(60000, 28, 28), containing the training data. Pixel values range\n",
            "  from 0 to 255.\n",
            "y_train:uint8NumPy array of digit labels (integers in range 0-9)\n",
            "  with shape(60000,)for the training data.\n",
            "x_test:uint8NumPy array of grayscale image data with shapes(10000, 28, 28), containing the test data. Pixel values range\n",
            "  from 0 to 255.\n",
            "y_test:uint8NumPy array of digit labels (integers in range 0-9)\n",
            "  with shape(10000,)for the test data.\n",
            "Example\n",
            "License:\n",
            "Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset,\n",
            "which is a derivative work from original NIST datasets.\n",
            "MNIST dataset is made available under the terms of theCreative Commons Attribution-Share Alike 3.0 license.\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.datasets.mnist.load_data(path=\"mnist.npz\")\n",
            "\n",
            "(x_train,y_train),(x_test,y_test)=keras.datasets.mnist.load_data()assertx_train.shape==(60000,28,28)assertx_test.shape==(10000,28,28)asserty_train.shape==(60000,)asserty_test.shape==(10000,)\n",
            "\n",
            "\n",
            "\n",
            "Now you know everything there is to know about using built-in training loops and\n",
            "writing your own from scratch.\n",
            "To conclude, here's a simple end-to-end example that ties together everything\n",
            "you've learned in this guide: a DCGAN trained on MNIST digits.\n",
            "\n",
            "\n",
            "Code:\n",
            "\n",
            "\n",
            "Loads the Fashion-MNIST dataset.\n",
            "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories,\n",
            "along with a test set of 10,000 images. This dataset can be used as\n",
            "a drop-in replacement for MNIST.\n",
            "The classes are:\n",
            "Returns\n",
            "Tuple of NumPy arrays:(x_train, y_train), (x_test, y_test).\n",
            "x_train:uint8NumPy array of grayscale image data with shapes(60000, 28, 28), containing the training data.\n",
            "y_train:uint8NumPy array of labels (integers in range 0-9)\n",
            "  with shape(60000,)for the training data.\n",
            "x_test:uint8NumPy array of grayscale image data with shapes\n",
            "  (10000, 28, 28), containing the test data.\n",
            "y_test:uint8NumPy array of labels (integers in range 0-9)\n",
            "  with shape(10000,)for the test data.\n",
            "Example\n",
            "License:\n",
            "The copyright for Fashion-MNIST is held by Zalando SE.\n",
            "Fashion-MNIST is licensed under theMIT license.\n",
            "\n",
            "\n",
            "Code:\n",
            "keras_core.datasets.fashion_mnist.load_data()\n",
            "\n",
            "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()assertx_train.shape==(60000,28,28)assertx_test.shape==(10000,28,28)asserty_train.shape==(60000,)asserty_test.shape==(10000,)\n",
            "\n",
            " \n",
            "\n",
            "RAG Response: The MNIST dataset is a collection of 70,000 grayscale images of handwritten digits (0-9). It's divided into 60,000 training images and 10,000 test images. Each image is 28x28 pixels. The MNIST digits classification dataset is a popular benchmark for image classification tasks. Here's how you can load the MNIST dataset using Keras:\n",
            "\n",
            "```python\n",
            "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
            "```\n",
            "        \n",
            "        This code loads both the training and testing splits of the MNIST dataset. \n",
            "\n",
            "Normal Response: **MNIST (Modified National Institute of Standards and Technology)** is a large dataset of handwritten digits that is commonly used for training and evaluating machine learning models.\n",
            "\n",
            "**Characteristics:**\n",
            "\n",
            "* **Size:** 70,000 grayscale images, divided into 60,000 training images and 10,000 test images.\n",
            "* **Dimensions:** Each image is 28 x 28 pixels, resulting in a 784-dimensional feature vector.\n",
            "* **Labels:** Each image is labeled with a digit from 0 to 9.\n",
            "* **Variations:** The dataset includes variations such as skewing, rotation, and noise, making it a more realistic dataset for training.\n",
            "\n",
            "**Usage:**\n",
            "\n",
            "* **Image classification:** MNIST is primarily used for training models to classify handwritten digits.\n",
            "* **Benchmarking:** It is a standard benchmark dataset for evaluating the performance of machine learning algorithms on image classification tasks.\n",
            "* **Unsupervised learning:** The dataset can also be used for unsupervised learning tasks such as clustering and dimensionality reduction.\n",
            "\n",
            "**Why MNIST is Popular:**\n",
            "\n",
            "* **Simplicity:** The images are simple and easy to understand, making it suitable for beginners.\n",
            "* **Well-defined task:** The task of classifying handwritten digits is well-defined and has a clear objective.\n",
            "* **Large and diverse:** The dataset is large and contains a wide variety of samples, making it a good representation of real-world scenarios.\n",
            "* **Historical significance:** MNIST has been used for decades in machine learning research and is widely recognized as a benchmark dataset.\n",
            "\n",
            "**Applications:**\n",
            "\n",
            "MNIST has been used in various applications, including:\n",
            "\n",
            "* Handwritten digit recognition systems\n",
            "* Document analysis\n",
            "* Image processing\n",
            "* Artificial intelligence research\n",
            "* Educational tools \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3):**\n",
            "\n",
            "1. **Relevance to Keras Documentation (0.6):** The response provides a single line of code to load the MNIST dataset, which is relevant to Keras documentation. However, it does not provide much context or further explanation about how this code snippet relates to the use of Keras.\n",
            "\n",
            "2. **Accuracy (1.0):** The code provided correctly loads the MNIST dataset in Keras.\n",
            "\n",
            "3. **Clarity (1.0):** The response is clear and concise, providing a single code example without unnecessary information.\n",
            "\n",
            "Total score: 2.6/3\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Question: What is the purpose of using the Embedding layer in Keras? \n",
            "\n",
            "Context Retrieved: In the mixed dimension embedding technique, we train embedding vectors with full dimensions\n",
            "for the frequently queried items, while train embedding vectors withreduced dimensionsfor less frequent items, plus aprojection weights matrixto bring low dimension embeddings\n",
            "to the full dimensions.\n",
            "More precisely, we defineblocksof items of similar frequencies. For each block,\n",
            "ablock_vocab_size X block_embedding_dimembedding table andblock_embedding_dim X full_embedding_dimprojection weights matrix are created. Note that, ifblock_embedding_dimequalsfull_embedding_dim,\n",
            "the projection weights matrix becomes anidentitymatrix. Embeddings for a given batch of itemindicesare generated via the following steps:\n",
            "\n",
            "\n",
            "Code:\n",
            "classMDEmbedding(keras.layers.Layer):def__init__(self,blocks_vocabulary,blocks_embedding_dims,base_embedding_dim,name=None):super().__init__(name=name)self.num_blocks=len(blocks_vocabulary)# Create vocab to block lookup.keys=[]values=[]forblock_idx,block_vocabinenumerate(blocks_vocabulary):keys.extend(block_vocab)values.extend([block_idx]*len(block_vocab))self.vocab_to_block=tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys,values),default_value=-1)self.block_embedding_encoders=[]self.block_embedding_projectors=[]# Create block embedding encoders and projectors.foridxinrange(self.num_blocks):vocabulary=blocks_vocabulary[idx]embedding_dim=blocks_embedding_dims[idx]block_embedding_encoder=embedding_encoder(vocabulary,embedding_dim,num_oov_indices=1)self.block_embedding_encoders.append(block_embedding_encoder)ifembedding_dim==base_embedding_dim:self.block_embedding_projectors.append(layers.Lambda(lambdax:x))else:self.block_embedding_projectors.append(layers.Dense(units=base_embedding_dim))defcall(self,inputs):# Get block index for each input item.block_indicies=self.vocab_to_block.lookup(inputs)# Initialize output embeddings to zeros.embeddings=tf.zeros(shape=(tf.shape(inputs)[0],base_embedding_dim))# Generate embeddings from blocks.foridxinrange(self.num_blocks):# Lookup embeddings from the current block.block_embeddings=self.block_embedding_encoders[idx](inputs)# Project embeddings to base_embedding_dim.block_embeddings=self.block_embedding_projectors[idx](block_embeddings)# Create a mask to filter out embeddings of items that do not belong to the current block.mask=tf.expand_dims(tf.cast(block_indicies==idx,tf.dtypes.float32),1)# Set the embeddings for the items not belonging to the current block to zeros.block_embeddings=block_embeddings*mask# Add the block embeddings to the final embeddings.embeddings+=block_embeddingsreturnembeddings\n",
            "\n",
            "\n",
            "\n",
            "Layers are the basic building blocks of neural networks in Keras.\n",
            "A layer consists of a tensor-in tensor-out computation function (the layer'scallmethod)\n",
            "and some state, held in \"variables\" (the layer'sweights).\n",
            "A Layer instance is callable, much like a function:\n",
            "Unlike a function, though, layers maintain a state, updated when the layer receives data\n",
            "during training, and stored inlayer.weights:\n",
            "\n",
            "\n",
            "Code:\n",
            "importkeras_coreaskerasfromkeras_coreimportlayerslayer=layers.Dense(32,activation='relu')inputs=keras.random.uniform(shape=(10,20))outputs=layer(inputs)\n",
            "\n",
            ">>>layer.weights[<Variable'dense/kernel:0'shape=(20,32)dtype=float32>,<Variable'dense/bias:0'shape=(32,)dtype=float32>]\n",
            "\n",
            "\n",
            "\n",
            "Layers are the basic building blocks of neural networks in Keras.\n",
            "A layer consists of a tensor-in tensor-out computation function (the layer'scallmethod)\n",
            "and some state, held in \"variables\" (the layer'sweights).\n",
            "A Layer instance is callable, much like a function:\n",
            "Unlike a function, though, layers maintain a state, updated when the layer receives data\n",
            "during training, and stored inlayer.weights:\n",
            "\n",
            "\n",
            "Code:\n",
            "importkeras_coreaskerasfromkeras_coreimportlayerslayer=layers.Dense(32,activation='relu')inputs=keras.random.uniform(shape=(10,20))outputs=layer(inputs)\n",
            "\n",
            ">>>layer.weights[<Variable'dense/kernel:0'shape=(20,32)dtype=float32>,<Variable'dense/bias:0'shape=(32,)dtype=float32>]\n",
            "\n",
            " \n",
            "\n",
            "RAG Response: The Embedding layer in Keras is used to convert categorical features into dense vectors. This is useful for tasks like natural language processing or recommender systems, where each category is represented by a unique integer. For example:\n",
            "\n",
            "        ```python\n",
            "        model = keras.Sequential([\n",
            "            keras.Input(shape=(None,)),\n",
            "            keras.layers.Embedding(1000, 128),\n",
            "            keras.layers.Dense(64),\n",
            "        ])\n",
            "        ```\n",
            "        In this example, the Embedding layer converts integers representing words into dense vectors, allowing for downstream processing in the neural network. \n",
            "\n",
            "Normal Response: The purpose of using the Embedding layer in Keras is to convert categorical or ordinal features into dense vectors. This is a crucial step in many deep learning models that handle data with categorical features, as it allows the model to learn meaningful representations of these features and utilize them effectively in downstream tasks.\n",
            "\n",
            "Here are the key purposes of using the Embedding layer in Keras:\n",
            "\n",
            "1. **Feature Representation:** The Embedding layer transforms each unique value in a categorical feature into a dense vector of a specified dimension. This vector representation captures the semantic meaning and relationships between the different categories. It allows the model to treat categorical features as numerical inputs, making them suitable for mathematical operations and gradient-based training.\n",
            "\n",
            "2. **Efficient Encoding:** Embedding layers provide an efficient way to encode categorical features that have a large number of unique values. Instead of creating one-hot encoded vectors for each categorical value, the Embedding layer assigns a dense vector to each value, saving computational resources and reducing the dimensionality of the input data.\n",
            "\n",
            "3. **Categorical Feature Learning:** By using an Embedding layer, the model can automatically learn the relationships and similarities between different categories. This is particularly useful for tasks like natural language processing, where words or phrases often have semantic connections that can be captured by the embedding vectors.\n",
            "\n",
            "4. **Transfer Learning:** Pre-trained embedding models, such as word embeddings trained on large text corpora, can be incorporated into your Keras model using the Embedding layer. This allows you to leverage existing knowledge and improve the performance of your model on downstream tasks, especially for text data.\n",
            "\n",
            "5. **Improved Generalization:** By representing categorical features as dense vectors, the model is better able to generalize to unseen data. The embedding vectors capture the semantic relationships between categories, enabling the model to make more accurate predictions on new examples that may not have been encountered during training.\n",
            "\n",
            "Overall, the Embedding layer in Keras is a powerful tool that enhances the representation and utilization of categorical features in deep learning models. It allows for efficient encoding, facilitates feature learning, supports transfer learning, and improves the generalization capabilities of the model. \n",
            "\n",
            "Verify Response: - **RAG Response Total Score (out of 3):**\n",
            "\n",
            "  1. Relevance to Keras Documentation: 1 (The response only briefly mentions the Embedding layer in Keras, but it does not go into detail about its benefits or use cases.)\n",
            "  2. Accuracy: 1 (The response is factually correct, but it does not provide a comprehensive explanation of the Embedding layer.)\n",
            "  3. Clarity: 1 (The response is clear, but it lacks depth and useful examples.)\n",
            "   Total Score: 3/9\n",
            "\n",
            "- **Normal Model Response Total Score (out of 3):**\n",
            "\n",
            "  1. Relevance to Keras Documentation: 1 (The response touches upon the Embedding layer in Keras, but it does not provide any specific examples or direct references to the documentation.)\n",
            "  2. Accuracy: 1 (The response correctly describes the purpose and benefits of the Embedding layer.)\n",
            "  3. Clarity: 1 (The response is written in a clear and concise manner.)\n",
            "   Total Score: 3/9\n",
            "\n",
            "- **Better Response:**\n",
            "\n",
            "The Normal Model Response is the better of the two, as it provides a more comprehensive explanation of the Embedding layer and its advantages in Keras. However, both responses lack specific examples taken directly from the Keras documentation, which could have improved their relevance and clarity.\n",
            "\n",
            "In the future, it would be ideal for a response to include direct references to the Keras documentation and actual code examples highlighting the use of the Embedding layer. By making the response more closely aligned with the Keras documentation, it would provide the user with a clearer and more accurate understanding of the topic. \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "### 1. **Fine-tuning an LLM vs. Fine-tuning Other Models:**\n",
        "   It is better to focus on fine-tuning the LLM itself for the KerasInsight project. Since the core of the project involves extracting relevant information from large text corpora, LLMs are ideally suited for this task due to their ability to understand and generate human-like text based on vast amounts of data. Fine-tuning an LLM is the most effective approach, given that the project revolves around handling documentation and answering complex queries. Fine-tuning other models may not be as relevant, unless specific optimizations (like search ranking) are required, but in this case, the LLM should be the primary focus.\n",
        "\n",
        "### 2. **Evaluation Metrics:**\n",
        "   Using the **RAG triad** (Answer Relevance, Context Relevance, Groundedness) is the most suitable choice for this task. Unlike metrics such as ROUGE or BLEU that are more suited for text summarization or translation tasks, the RAG metrics are tailored for evaluating how accurate, relevant, and grounded the responses are to the source material. Since the goal is to provide grounded answers specific to Keras documentation, RAG metrics are the best fit.\n",
        "\n",
        "   Additionally, assessing whether the combination of **RAG + LLM** enhances response quality versus the LLM alone is a useful approach. Verifying groundedness by comparing responses from different LLMs (e.g., using Gemini alongside other LLMs) adds robustness to the evaluation process.\n",
        "\n",
        "### 3. **Use Case and Flexibility for Documentation Updates:**\n",
        "   The design of KerasInsight to handle websites with frequently changing documentation is a key strength. Unlike static LLMs that are trained once and might miss updated content, this system scrapes the latest data and dynamically builds a vector store for similarity search. This ensures that users get up-to-date information, giving the system a significant advantage over models relying on outdated datasets.\n",
        "\n",
        "### 4. **Potential as a Chatbot for Documentation:**\n",
        "   The project can easily evolve into a chatbot for Keras or other similar documentation platforms. Given that it dynamically fetches and embeds content, the system can be extended to a wide range of documentation-heavy use cases such as developer platforms, APIs, etc. The ability to adapt to changing documentation environments makes this project highly valuable for real-time information retrieval.\n",
        "\n",
        "### 5. Scope for Improvement:\n",
        "  While the generation of responses has been good, there are areas where retrieval and chunking strategies can be further improved. Using more sophisticated embedding models and experimenting with different chunking strategies can enhance the accuracy and relevance of the retrieved information. Future work should focus on optimizing these aspects to handle even more complex queries and improve response precision\n",
        "\n",
        "### Final Points:\n",
        "   - **Fine-tuning the LLM** was the right choice for the KerasInsight project.\n",
        "   - The **RAG triad metrics** are a well-suited and effective way to evaluate the model's performance.\n",
        "   - Testing the effectiveness of **RAG + LLM** versus LLM alone, and verifying groundedness using another LLM, adds value to the evaluation process.\n",
        "   - The **flexibility to handle dynamic documentation** makes the system superior to static LLMs for real-time tasks.\n",
        "   - The project can be scaled into a chatbot solution for other documentation-heavy fields as well.\n",
        "   - Further improvements in embedding models and chunking strategies will enhance retrieval accuracy and system robustness."
      ],
      "metadata": {
        "id": "tQBoInh4DzrN"
      }
    }
  ]
}